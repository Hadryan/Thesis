\chapter{Conclusion}
The success of today's supervised machine learning algorithms in complex tasks depends strongly on the availability of large scale high quality labeled data. In practice, however, for many applications and domains, the available training data is limited or noisy and it is difficult to build machines that can learn with such imperfect training data.
%
In contrast, humans are capable of uncovering the underlying concepts, relations, and structure of sparsely observed data with variable quality and use that knowledge to go far beyond the scarcity of the data and routinely make successful generalizations based on them.

The argument of the \emph{poverty of stimulus}~\citep{chomsky1980rules} in human learning suggests that the observed data is not rich enough for selecting a correct target hypothesis without postulating an a priori knowledge~\citep{chomsky1971problems}.
%
In line with this, when designing machine learning algorithms, pure \emph{data-driven} learning, which relies only on previous experience, does not seem to be able to learn generalizable solutions~\citep{Mitchell80theneed}. Similar to human's \emph{innately primed} learning, having part of the knowledge encoded in the learning algorithms in the form of strong or weak biases, can help them learn solutions that better generalize to unseen samples~\citep{Mitchell:1997:ML}.

In this thesis, we focused on this problem of the poverty of stimulus for learning algorithms. We argued that even noisy and limited signals can contain a great deal of valid information that can be incorporated along with prior knowledge and biases that are encoded into learning algorithms in order to solve complex problems. 
We study how to improve the learning with imperfect supervision signals in the context of language understanding and sequence modeling tasks.


\section{Research Questions and Conclusions}
We referred to ``\emph{imperfect supervision}'' as a general term covering a variety of situations where the learning process is based on imperfect training examples. This imperfection can be in the number or coverage of training examples like learning from \emph{incomplete supervision}, where only a limited subset of data is labeled or no labeled data is available. The imperfection can also refer to the labeling process like in \emph{inexact supervision} where only coarse-grained annotations are provided or \emph{inaccurate supervision} where the given labels are noisy and they are not always ground truth~\citep{zhou2018brief}. We also consider situations where not only the labels in the training data but also feature vectors can be limited, noisy, or subject to change over time.

We formulated the main research question of the thesis as:
%
\resq{main}
%

We broke down our main research question into three questions and addressed each of them in each part of this thesis.
The first question, that we addressed in Part~\ref{part1} of the thesis, was:
%
\resq{p1}

In Part~\ref{part1}, we focused on learning representations for the entities and abstract concepts, like topical relevance, a political conviction, etc., given data in which the features and labels can be noisy or subject to change over time.

First, in Chapter~\ref{chap:2} informed by a discussion of the early work by~\citet{Luhn:1958} about \emph{significant words} we introduced \emph{\SWLMs (\acswlms)} that estimate a representation for an entity or concept that is associated with a set of textual documents in a way that the estimated representation captures significant features by avoiding the distracting effect of common features as well as rare features. We evaluated \acswlms on a set of problems including (pseudo-)relevance feedback in document ranking and group profiling in contextual suggestion and recommendation, and showed that we can improve the quality and robustness of representations by making them dependent less on general or specific features, but rely more on significant features.

Then, in Chapter~\ref{chap:3}, we extended our discussions in Chapter~\ref{chap:2} to hierarchical structures and introduced \emph{\HSWLMs (\achswlms)} for estimating separable representations for hierarchical entities. We demonstrated that based on ranking and classification principles, the \emph{separation property} in the data representation is a desirable foundational property which leads to separability of scores and consequently improves the accuracy of classifiers' decisions. 

We showed that in order to have horizontally and vertically separable representations for hierarchically structured data, they should capture all, and only, the essential features of the entities taking their position in the hierarchy into account, which is the key idea behind \achswlms. We evaluated the performance of classification over time using separable representations learned by \achswlms and showed that separability makes the model more robust and transferable over time by filtering out non-essential and non-stable features.

\textbf{The main conclusion of Part~\ref{part1}} is that incorporating prior knowledge can help to improve the robustness of the outcome of the learning process in noisy and variable environments. We showed that taking the general structure of the data as prior knowledge, which is, in fact, a form of inductive bias, can help to learn representations that are not only effective but also less affected by noisy factors in the data.

\bigskip
The second question, which we addressed in Part~\ref{part2} of the thesis, was:
%
\resq{p2}

In Part~\ref{part2}, we focused on how to augment the training data using a vast amount of data that are not hand-labeled, but weakly annotated using, for instance, a heuristic function. We discuss how to train learning algorithms using such weak labels and how to learn properties of the weakly annotated data, like the quality of labels and incorporate those in the learning process.  

First, in Chapter~\ref{chap:4}, we proposed to use unsupervised methods in order to programmatically generate large amounts of training data~\citep{Ratner:2016}, as weakly annotated data, to train effective neural ranking models. We focus on the task of assessing the topical relevance, i.e., ranking documents given a query. We examined various neural ranking models with different ranking architectures and objectives, and different input representations.

We found that providing the network with raw data and letting the network learn the features that matter, gives the network a chance of learning how to ignore imperfection in the training data. Also we showed in the case of having weakly annotated training data, by targeting some explicit labels from the data, we may end up with a model that has learned to express the data very well, but is incapable of going beyond it. We also observed that when learning from weakly annotated data, it is crucial to provide the network with a considerable amount of diverse training examples to help the model learn at the edge of its capacity.

Then, in Chapter~\ref{chap:5}, we proposed a set of systematic approaches that are tasks and architecture independent and can meta-learn the quality of the labels and explicitly control the learning process with respect to the estimated qualities. We introduced two semi-supervised learning approaches in the presence of weakly labeled data: \emph{Learning from Controlled Weak Supervision (\cws)} and \emph{\fwlfulllc (\fwl)}.
\cws is a meta-learning approach that unifies learning to estimate the confidence score of weak annotations and training neural networks to learn a target task with controlled weak supervision, i.e., using weak labels to updating the parameters but taking their estimated confidence scores into account. \fwl is a student-teacher framework in which the student network is in charge of learning a target task given a vast amount of samples with weak labels associated with fidelity scores that are generated by the teacher network.  We applied both \cws and \fwl to document ranking and sentiment classification and empirically verified that they improve the learning process in terms of performance and convergence time. 

\textbf{The main conclusion of part~\ref{part2}} is that we can design models that are capable of learning from weakly annotated data by defining proper training objectives. We also found that we can use the data to meta-learn some properties of the data, like the quality of labels and incorporate these properties in the learning process.

\bigskip
The last question, that we addressed in Part~\ref{part3} of the thesis, was:
%
\resq{p3}

In Part~\ref{part3}, we focused on the sequence processing neural networks and studied the role of inductive biases, like recurrent inductive bias, on the generalization and data efficiency of these models on different language understanding and sequence modeling tasks.

In Chapter~\ref{chap:6}, we argued that the lack of recurrent inductive bias in feed-forward self-attentive models, like Transformer, can lead to the failure of the model on complex reasoning tasks with limited data, algorithmic tasks where length generalization over training samples is needed, and structured language understanding tasks. We proposed the Universal Transformer, a self-attentive concurrent-recurrent sequence model, in which we introduce recurrence in depth by repeatedly modifying a series of vector representations for each position of the sequence in parallel. The key idea of the universal transformer is sharing the parameters across the layers which forms a recurrent inductive bias in depth and saves massively on the number of parameters and leads to a more data efficient model. 

We also discussed other assumptions whose encoding them in a model as inductive biases can help to extrapolate from training data and to better generalizing at test time.  We also introduced some variants of the Universal Transformer with conditional computations and showed that it can achieve stronger results compared to the fixed-depth Universal Transformer.

\textbf{The main conclusion of Part~\ref{part3}} is that injecting inductive biases into learning algorithms can improve their data-efficiency and generalization. These inductive biases are in fact innate prior knowledge for the learning algorithms that can eventually help overcoming the challenging problem of the poverty of stimulus.

\subsection{Contributions}
Here is a list of main contributions of this thesis to solving problems in information retrieval, natural language processing, and machine learning:
\begin{itemize}
    \item We proposed approaches for learning \emph{robust} and \emph{time agnostic} representations for documents, given the relations in the data as prior knowledge.
    \item We presented a new family of models, significant words language models, that capture only and all essential features for representing a set of documents and showed they are not only easily inspectable by human, but also effective in many tasks, like classification, (pseudo)-relevance feedback for document ranking, and contextual suggestion. 
    \item We proposed using heuristic unsupervised models as weak labelers to create a large-scale weakly annotated training set, for tasks where no such training set is available like document ranking. We further studied the effectiveness of different general architectures for neural rankers and various objective functions when learning with weak supervision. This provided excellent groundwork for many researchers to apply deep neural networks on search and ranking problems.
    \item We proposed approaches for learning to learn from weak supervision, by jointly optimizing for the objectives of the main task as well as learning and incorporating fidelity of labels. The proposed ideas have been used by many researchers on applications/tasks where the labels in the training set are of variable qualities.  
    \item We proposed Turing complete version of the transformer model, the universal transformer, in which we introduce recurrence-in-depth that increases generalization and effectiveness of the model due to the introduced recurrent inductive bias. We also used adaptive computation and sowed effectiveness of efficiency of the model in several sequence modeling tasks. 
\end{itemize}


\bigskip
As the \textbf{general conclusion} of this thesis, we found that we can improve the process of learning with imperfect supervision by (i) \emph{employing  prior knowledge in learning algorithms} (Part~\ref{part1}), (ii) \emph{augmenting data and learning to learn how to better use the data} (Part~\ref{part2}), and (iii) \emph{introducing inductive biases to learning algorithms} (Part~\ref{part3}). 
%
These general ideas are, in fact, the key ingredients for building any learning algorithms that can generalize beyond (imperfections in) the observed data ~\citep{Mitchell80theneed}.

We focused on language understanding tasks, like assessing relevance on textual documents, machine translation, question answering, and natural language reasoning. To solve these tasks, we developed ideas that lead to not only better learning ``process'', i.e. better models, but also better ``product'', i.e. better outcomes. 







\section{Towards Building Machines that Deal with the Poverty of Stimulus}

There has been a long discussion between empiricists and rationalists in the context of human learning concerning the extent to which we are dependent on our experiences and observations in our effort to gain knowledge~\citep{markie2004rationalism}.  
%
Empiricists claim that sensory observations (data) are the ultimate source of all our concepts and knowledge, while rationalists claim that there are significant ways in which our concepts and knowledge are gained independently of our experiences and observations. 

A similar discussion has been raised in machine learning in the context of learning algorithms.
%
Some machine learning researchers believe that the intrinsic complexity of the world means we should not build any prior knowledge into our systems: ``seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.''%\citep{TheBitterLesson}
\footnote{``The Bitter Lesson'' by Rich Sutton: \url{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}}
%
Other machine learning researchers, on the other hand, emphasize the fact that ``there are no predictions without assumptions, no generalization without inductive bias'' and believe that the complexity of the world, as a matter of fact, leads to crippling intractability for the approaches on which empiricists proposes to rely and argue that only with the right prior knowledge and the right inductive biases, we can get a handle on that complexity.
%~\citep{maxblogpost, Mitchell:1997:ML}.
\footnote{``Do we still need models or just more data and compute?'' by Max Welling: \url{https://staff.fnwi.uva.nl/m.welling/wp-content/uploads/Model-versus-Data-AI-1.pdf}}

In practice, we do not apply either rigorous empiricist or rationalist approaches in machine learning.  When we pursue more explanatory rationalist approaches, we always recognize the importance of observations and the data as the means by which reality affects our understanding.  When taking a data-driven approach, we make use of reasoning at least in the act of observing data, i.e., choices such as how data is selected, assumptions we make, biases to our algorithms, and the overall architectures of our solution.
%
In order to build machines that can create knowledge a truce between these two main approaches is essential.
There is no doubt that the success of deep learning is very much a success of scale and in general, machine learning methods that survive the test of time and make breakthrough progress tend to scale.
But incorporating knowledge or inductive biases has its own success stories and the question here is more about ``what'' that knowledge or biases should be and ``when'' and ``how'' it should be incorporated. 

While finding the right inductive biases is hard, they can enable progress on intractable problems or situations where we cannot rely on arbitrarily scaling of computation such as settings with noisy or limited data, which characterizes most real-world applications. 
Besides, scalability can be defined in many dimensions, if a method is ``scalable'' with more data and computation, it has a chance of succeeding only in a subset of problems that we can gather infinite data for them. 
However, we can define ``scale,'' as in ``scale to new problems,'' which is, in fact, the ability of generalization, where inductive biases can play crucial roles to achieve it, especially when considering the problem of poverty of stimulus.


\bigskip
% closing paragraph
We are enthusiastic about recent developments in machine learning models that are able to learn with imperfect supervision. 
By combining ideas on how to incorporate general prior knowledge, how to better use the data and meta learn its properties, and how to inject the right inductive biases, we hope that further improvements presented in this thesis will help us build learning algorithms that are more powerful, more data efficient, and more generalizable, and in a bigger picture, help machines to get closer to human-level intelligence. 
