\newcommand{\tfunc}{T} %function name for the teacher
\section{\fwlfull}
\label{sec:fidelity_weighted_learning}

In this section, we address the second research question of this chapter:
\resq{c5.2}

We introduce \fwlfull (\fwl), a Bayesian semi-supervised approach that leverages a small amount of data with strong labels to generate a larger training set with \emph{confidence-weighted weakly-labeled samples}, which can then be used to modulate the fine-tuning process based on the fidelity (or quality) of each weak sample. By directly modeling the inaccuracies introduced by the \wa in this way, we can control the extent to which we make use of this additional source of weak supervision: more for confidently-labeled weak samples close to the true observed data, and less for uncertain samples further away from the observed data. We use a non-parametric kernel-based method to measure the closeness.

We propose a setting consisting of two main modules. One is called the \std and is in charge of learning a suitable data representation and performing the main prediction task (similar to the \tnet in Section~\ref{sec:meta_learning}), the other is the \tch which modulates the learning process by modeling the inaccuracies in the labels. 

\input{03-part-02/chapter-05/figs_and_tables/fig_fwl_model.tex}
\subsection{Recipe of \fwlfull}
\label{sec:proposed-method}
In this section, we describe the \fwl approach for semi-supervised learning when we have access to weak supervision (e.g.\ heuristics or weak annotators). 

\input{03-part-02/chapter-05/figs_and_tables/alg_fwl.tex}
Our proposed setup comprises a neural network called the \textbf{\std} and a Bayesian function approximator called the \textbf{\tch}. The training process consists of three phases which we summarize in Algorithm~\ref{alg:main} and Figure~\ref{fig:model_fwl}.

\textbf{Step 1} \emph{Pre-train the \std on $\mathcal{D}_w$ using weak labels generated by the \wa}. 

The main goal of this step is to learn a \emph{task dependent} representation of the data as well as pretraining the \std. The \std function is a neural network consisting of two parts. The first part $\psi(.)$ learns the data representation and the second part $\phi(.)$ performs the prediction task (e.g. classification). Therefore the overall function is $\hat{y}=\phi(\psi(x_i))$. The \std is trained on all samples of the weak dataset $\mathcal{D}_w=\{(x_i, \tilde{y}_i)\}$. For brevity, in the following, we will refer to both data sample $x_i$ and its representation $\psi(x_i)$ by $x_i$ when it is obvious from the context. 
From the self-supervised feature learning point of view, we can say that representation learning in this step is solving a surrogate task of approximating the expert knowledge, for which a noisy supervision signal is provided by the \wa.  


\textbf{Step 2} \emph{Train the \tch on the strong data $(\psi(x_j),y_j) \in \mathcal{D}_s$ represented in terms of the student representation $\psi(.)$ and then use the \tch to generate a soft dataset $\mathcal{D}_{sw}$ consisting of $\langle \textrm{sample}, \textrm{predicted label}, \textrm{ confidence} \rangle$ for \textbf{all} data samples.} 

We use a Gaussian process as the \tch to capture the label uncertainty in terms of the \std representation, estimated w.r.t\ the strong data. A prior mean and co-variance function is chosen for $\mathcal{GP}$. The learned embedding function $\psi(\cdot)$ in Step 1 is then used to map the data samples to dense vectors as input to the $\mathcal{GP}$. 
We use the learned representation by the \std in the previous step to compensate lack of data in $\mathcal{D}_s$ and the \tch can enjoy the learned knowledge from the large quantity of the weakly annotated data. This way, we also let the \tch  see the data through the lens of the \std.

The $\mathcal{GP}$ is trained on the samples from $\mathcal{D}_s$ to learn the posterior mean $\bm{m}_{\rm post}$ (used to generate soft labels) and posterior co-variance $K_{\rm post}(.,.)$ (which represents label uncertainty).
%\begin{eqnarray*}
%\mathcal{GP}(\bm{m}_{\rm post}, K_{\rm post})&=&\mathcal{GP}(\bm{m}_{\rm %prior}, K_{\rm prior}) | \mathcal{D}_s=\{(\psi(x_j),y_j)\}\\
%\sshrink
%\end{eqnarray*}
We then create the \emph{soft dataset} $\mathcal{D}_{sw}=\{(x_t,\bar{y}_t)\}$ using the posterior $\mathcal{GP}$, input samples $x_t$ from $\mathcal{D}_w \cup \mathcal{D}_s$, and predicted labels $\bar{y}_t$ with their associated uncertainties as computed by $T(x_t)$ and $\Sigma(x_t)$:
\begin{eqnarray*}
\tfunc(x_t) &=& g(\bm{m}_{\rm post}(x_t))\\
\Sigma(x_t) &=& h(K_{\rm post}(x_t,x_t))
\sshrink
\end{eqnarray*}
The generated labels are called \emph{soft labels}. Therefore, we refer to $\mathcal{D}_{sw}$ as a soft dataset. $g(.)$ transforms the output of $\mathcal{GP}$ to the suitable output space. For example in classification tasks, $g(.)$ would be the softmax function to produce probabilities that sum up to one. 
For multidimensional-output tasks where a vector of variances is provided by the $\mathcal{GP}$, the vector $K_{\rm post}(x_t,x_t)$ is passed through an aggregating function $h(.)$ to generate a scalar value for the uncertainty of each sample. 
Note that we train $\mathcal{GP}$ only on the strong dataset $\mathcal{D}_s$ but then use it to generate soft labels $\bar{y}_t = \tfunc(x_t)$ and uncertainty $\Sigma(x_t)$ for samples belonging to $\mathcal{D}_{sw}=\mathcal{D}_w\cup \mathcal{D}_s$.

In practice, we furthermore divide the space of data into several regions and assign each region a separate $\mathcal{GP}$ trained on samples from that region. This leads to a better exploration of the data space and makes use of the inherent structure of data. The algorithm called clustered $\mathcal{GP}$ gave better results compared to a single GP. 

By this division of space, we take advantage of the knowledge learned by several teachers, each an expert on its specific region of data space, which helps in particular when the dimensionality of the input is rather high. As a nice side-effect, this also solves the scalability issues of $\mathcal{GP}$s in that we can increase the number of regions until the number of points in each region is tractable with a single $\mathcal{GP}$, and train these models in parallel. See Section~\ref{sec:CGP} for the detailed description of clustered $\mathcal{GP}$.

%
\textbf{Step 3} \emph{Fine-tune the weights of the \std network on the soft dataset, while modulating the magnitude of each parameter update by the corresponding \tch-confidence in its label.}

The \std network of Step 1 is fine-tuned using samples from the soft dataset $\mathcal{D}_{sw}=\{(x_t, \bar{y}_t)\}$ where $\bar{y}_t=\tfunc(x_t)$.
The corresponding uncertainty $\Sigma(x_t)$ of each sample is mapped to a confidence value according to Equation~\ref{eqn:eta2} below, and this is then used to determine the step size for each iteration of the stochastic gradient descent (SGD). So, intuitively, for data points where we have true labels, the uncertainty of the \tch is almost zero, which means we have high confidence and a large step-size for updating the parameters. However, for data points where the \tch is not confident, we down-weight the training steps of the \std. This means that at these points, we keep the \std function as it was trained on the weak data in Step 1.

More specifically, we update the parameters of the \std by training on $\mathcal{D}_{sw}$ using SGD:
\begin{eqnarray*}
%  \pmb{w}^* &=& \argmin_{\pmb{w} \in \mathcal{W}} \>   \mathcal{L}(\pmb{w}) \\
%  &:=& \frac{1}{T}\sum_{(x_t,\bar{y}_t) \in \mathcal{D}_{sw}}l(\pmb{w}, x_i, \bar{y}_i) + \mathcal{R}(\pmb{w}), \\
  \pmb{w}^* &=& \argmin_{\pmb{w} \in \mathcal{W}} \> \frac{1}{N}\sum_{(x_t,\bar{y}_t) \in \mathcal{D}_{sw}}l(\pmb{w}, x_t, \bar{y}_t) + \mathcal{R}(\pmb{w}), \\
  \pmb{w}_{t+1} &=& \pmb{w}_t - \eta_t(\nabla l(\pmb{w},x_t,\bar{y}_t) + \nabla \mathcal{R}(\pmb{w}))
\end{eqnarray*}
where $l(\cdot)$ is the per-example loss, $\eta_t$ is the total learning rate, $N$ is the size of the soft dataset $\mathcal{D}_{sw}$, $\pmb{w}$ is the parameters of the \std network, and $\mathcal{R(.)}$ is the regularization term. %Regularization term is the usual regularization used by optimization packages (e.g. weight decay). Therefore, we do not go into its details here.

We define the total learning rate as $\eta_t = \eta_1(t)\eta_2(x_t)$, where $\eta_1(t)$ is the usual learning rate of our chosen optimization algorithm that anneals over training iterations, and $\eta_2(x_t)$ is a function of the label uncertainty $\Sigma(x_t)$ that is computed by the \tch for each data point. Multiplying these two terms gives us the total learning rate. In other words, $\eta_2$ represents the \emph{fidelity} (quality) of the current sample, and is used to multiplicatively modulate $\eta_1$. Note that the first term does not necessarily depend on each data point, whereas the second term does. We propose
\begin{equation}
 \label{eqn:eta2}
 \eta_2(x_t) = \exp[-\beta \Sigma(x_t)],    
\end{equation}
to exponentially decrease the learning rate for data point $x_t$ if its corresponding soft label $\bar{y}_t$ is unreliable (far from a true sample). In Equation~\ref{eqn:eta2}, $\beta$ is a positive scalar hyper-parameter. Intuitively, small $\beta$ results in a \std which listens more carefully to the \tch and copies its knowledge, while a large $\beta$ makes the \std pay less attention to the \tch, staying with its initial weak knowledge. 
More concretely speaking, as $\beta \to 0$ \std places more trust in the labels $\bar{y}_t$ estimated by the \tch and the \std copies the knowledge of the \tch. On the other hand, as $\beta \to \infty$, \std puts less weight on the extrapolation ability of $\mathcal{GP}$ and the parameters of the \std are not affected by the correcting information from the \tch. 


\subsection{Multi-Teacher \fwl using Clustered GP}
\label{sec:CGP}
We suggest using several $\mathcal{GP}=\{GP_{c_i}\}$ to explore the entire data space more effectively. Even though inducing points and stochastic methods make $\mathcal{GP}$s more scalable we still observed poor performance when the entire dataset was modeled by a single $\mathcal{GP}$. Therefore, the reason for using multiple $\mathcal{GP}$s is mainly empirical inspired by ~\citep{shen2006fast} which is explained in the following:

We used Sparse Gaussian Process implemented in GPflow. The algorithm is scalable in the sense that it is not $O(N^3)$ as original $\mathcal{GP}$ is. It introduces inducing points in the data space and defines a variational lower bound for the marginal likelihood. The variational bound can now be optimized by stochastic methods which make the algorithm applicable in large datasets. However, the tightness of the bound depends on the location of inducing points which are found through the optimization process. 

We empirically observed that a single $\mathcal{GP}$ does not give a satisfactory accuracy on left-out test dataset. We hypothesized that this can be due to the inability of the algorithm to find good inducing points when the number of inducing points is restricted to just a few.
Then we increased the number of inducing points $M$ which trades off the scalability of the algorithm because it scales with $O(NM^2)$. Moreover, apart from scalability which is partly solved by stochastic methods, we argue that the structure of the entire space may not be explored well by a single $\mathcal{GP}$ and its inducing points.
We guess this can be due to the observation that our datasets are distributed in a highly sparse way within the high dimensional embedding space. 
We also tried to cure the problem by means of PCA to reduce input dimensions and give a denser representation, but it did not result in a considerable improvement\citep{dehghani:2018:ICLR}. 
%The results are presented in Tabel~\ref{tbl_cgp}. 
%\input{cgp_res.tex}

We may be able to argue that clustered $\mathcal{GP}$ makes better use of the data structure roughly close to the idea of KISS-GP~\citep{Wilson:2015:KIS:3045118.3045307}.
In inducing point methods, it is normally assumed that $M\ll N$ ($M$ is the number of inducing points and $N$ is the number of training samples) for computational and storage saving. However, we have this intuition that few number of inducing points make the model unable to explore the inherent structure of data. By employing several GPs, we were able to use a large number of inducing points even when $M>N$ ($M$ is the total number of inducing points) which seemingly better exploits the structure of datasets. Because our work was not aimed to be a close investigation of GP, we considered clustered $\mathcal{GP}$ as the engineering side of the work which is a tool to give us a measure of confidence. Other tools such as a single $\mathcal{GP}$ with inducing points that form a Kronecker or Toeplitz covariance matrix are also conceivable. Therefore, we do not of course claim that we have proposed a new method of inference for GPs. 

Here is practical description of clustered $\mathcal{GP}$ algorithm:\\
{\it Clustered $\mathcal{GP}$}: Let $N$ be the size of the dataset on which we train the \tch. Assume we allocate $K$ teachers to the entire data space. Therefore, each $\mathcal{GP}$ sees a dataset of size $n=N/K$.
Then we use a simple clustering method (e.g. k-means) to find centroids of $K$ clusters $C_1, C_2, \ldots, C_K$ where $C_i$ consists of samples $\{x_{i,1}, x_{i,2},\ldots,x_{i,n}\}$. We take the centroid $c_i$ of cluster $C_i$ as the representative sample for all its content. Note that $c_i$ does not necessarily belong to $\{x_{i,1}, x_{i,2},...,x_{i,n}\}$. We assign each cluster a $\mathcal{GP}$ trained by samples belonging to that cluster. More precisely, cluster $C_i$ is assigned a $\mathcal{GP}$ whose data points are $\{x_{i,1}, x_{i,2},...,x_{i,n}\}$.
Because there is no dependency among different clusters, we train them in parallel to speed-up the procedure more. 

The pseudo-code of the clustered $\mathcal{GP}$ is presented in Algorithm~\ref{alg:CGP}. When the main issue is computational resources (when the number of inducing points for each $\mathcal{GP}$ is large), we can first choose the number $n$ which is the maximum size of the dataset on which our resources allow to train a $\mathcal{GP}$, then find the number of clusters $K=N/n$ accordingly. The rest of the algorithm remains unchanged. 
\input{03-part-02/chapter-05/figs_and_tables/alg_cgp.tex}


\subsection{FWL on a Toy Example}
\label{sec:toy_exmpale}
\input{03-part-02/chapter-05/figs_and_tables/fig_fwl_toy_example.tex}
To better understand \fwl, we apply \fwl to a one-dimensional toy problem to illustrate the various steps.
%
Let $f_t(x)=\sin(x)$ be the true function (red dotted line in Figure~\ref{fig:toy_plot1}) from which a small set of observations $\mathcal{D}_s=\{x_j,y_j\}$ is provided (red points in Figure~\ref{fig:toy_plot2}). These observation might be noisy, in the same way that labels obtained from a human labeler could be noisy.
%
A \wa function $f_{w}(x)=2sinc(x)$ (magenta line in Figure~\ref{fig:toy_plot1}) is provided, as an approximation to $f_t(.)$.

%
The task is to obtain a good estimate of $f_t(.)$ given the set $\mathcal{D}_s$ of strong observations and the \wa function $f_{w}(.)$.
%
We can easily obtain a large set of observations $\mathcal{D}_w=\{x_i,\tilde{y}_i\}$ from $f_{w}(.)$ with almost no cost (magenta points in Figure~\ref{fig:toy_plot1}). 

As the \tch, we use standard Gaussian process regression\footnote{\url{http://gpflow.readthedocs.io/en/latest/notebooks/regression.html}} with this kernel:
\begin{equation}
k(x_i,x_j)=k_{\rm RBF}(x_i,x_j)+k_{\rm White}(x_i,x_j)
\end{equation}
where,
\begin{flalign*}
    \hspace{6em}
    &&k_{\rm RBF}(x_i,x_j) &= \exp{\left(\frac{\Vert x_i-x_j\Vert^2}{2^2}\right)} & 
    \\
    &&k_{\rm White}(x_i,x_j) &= constant\_value, \quad \forall x_1=x_2 \text{ and } 0 \text{ otherwise} & 
\end{flalign*}

We fit only one $\mathcal{GP}$ on all the data points (i.e. no clustering). Also during fine-tuning, we set $\beta = 1$.
The \std is a simple feed-forward network with the depth of 3 layers and width of 128 neurons per layer.  We have used $tanh$ as the nonlinearity for the intermediate layers and a linear output layer. As the optimizer, we used Adam~\citep{Kingma:2014} and the initial learning rate has been set to $0.001$.
We randomly sample 100 data points from the \wa and 10 data points from the true function. We introduce a small amount of noise to the observation of the true function to model the noise in the human labeled data. 


We consider two experiments: 
\begin{enumerate}[leftmargin=*]
%save some space
\setlength{\topsep}{0.3pt}
\setlength{\partopsep}{0.3pt}
\setlength{\itemsep}{0.3pt}
\setlength{\parskip}{0.3pt}
\setlength{\parsep}{0.3pt}
    \item A neural network trained on weak data and then fine-tuned on strong data from the true function, which is the most common semi-supervised approach (Figure~\ref{fig:toy_plot3}).
    \item A teacher-student framework working by the proposed \fwl approach.
\end{enumerate} 

As can be seen in Figure~\ref{fig:toy_plot4}, \fwl by taking into account label confidence, gives a better approximation of the true hidden function.  We repeated the above experiment 10 times. The average RMSE with respect to the true function on a set of test points over those 10 experiments for the \std, were as follows:
\begin{enumerate}[leftmargin=*]
%save some space
\setlength{\topsep}{0.3pt}
\setlength{\partopsep}{0.3pt}
\setlength{\itemsep}{0.3pt}
\setlength{\parskip}{0.3pt}
\setlength{\parsep}{0.3pt}
    \item Student is trained on weak data (blue line in Figure~\ref{fig:toy_plot1}): $0.8406$,
    \item Student is trained on weak data then fine tuned on true observations (blue line in Figure~\ref{fig:toy_plot3}): $0.5451$,
    \item Student is trained on weak data, then fine tuned by soft labels and confidence information provided by the teacher (blue line in Figure~\ref{fig:toy_plot4}): $0.4143$ (best).
\end{enumerate}


\subsection{Connection with Vapnik's LUPI}
\label{sec:LUPI}
In this section, we highlight the connections of our work with Vapnik's \emph{learning using privileged information} (LUPI)~\citep{vapnik2009new, vapnik2015learning}. \fwl makes use of information from a small set of correctly labeled data to improve the performance of a semi-supervised learning algorithm. The main idea behind LUPI comes from the fact that humans learn much faster than machines. This can be due to the role that an \emph{Intelligent Teacher} plays in human learning. In this framework, the training data is a collection of triplets
\begin{equation}
    \{(x_1, y_1, x_1^*),\ldots,(x_n,y_n,x_n^*)\}\mathtt{\sim}P^n(x,y,x^*)
\end{equation}
where each ${(x_i,y_i)}$ is a pair of feature-label and $x_i^*$ is the additional information provided by an intelligent teacher to ease the learning process for the \std. Additional information for each ${(x_i,y_i)}$ is available only during training time and the learning machine must only rely on $x_i$ at test time. The theory of LUPI studies how to leverage such a teaching signal $x_i^*$ to outperform learning algorithms utilizing only the normal features $x_i$. For example, MRI brain images can be augmented with high-level medical or even psychological descriptions of Alzheimer's disease to build a classifier that predicts the probability of Alzheimer's disease from an MRI image at test time.
It is known from statistical learning theory~\citep{Vapnik1998} that the following bound for test error is satisfied with probability $1-\delta$:
\begin{equation}
\label{eq:LUPI_error_bound}
    R(f) \leq R_n(f) + O\left(\left(\frac{|\mathcal{F}|_{VC}-\log \sigma}{n}\right)^\alpha\right),
\end{equation}
where $R_n(f)$ denotes the training error over $n$ samples, $|\mathcal{F}|_{VC}$ is the VC dimension of the space of functions from which $f$ is chosen, and $\alpha \in [0.5,1]$. When the classes are not $separable$, $\alpha = 0.5$ i.e. the machine learns at a slow rate of $O(n^{-1/2})$. For easier problems where classes are $separable$, $\alpha=1$ resulting in a learning rate of $O(n^{-1})$.  The difference between these two cases is severe. The same error bound achieved for a separable problem with 10 thousand data points is only obtainable for a non-separable problem when 100 million data points are provided. This is prohibitive even when obtaining large datasets is not so costly. The theory of LUPI shows that an intelligent teacher can reduce $\alpha$ resulting in a faster learning process for the student. In this paper, we proposed a \emph{teacher}-\emph{student} framework for semi-supervised learning. Similar to LUPI, in \fwl a student is supposed to solve the main prediction task while an intelligent teacher provides additional information to improve its learning. In addition, we first train the \std network so that it obtains initial knowledge of weakly labeled data and learns a good data representation. Then the \tch is trained on truly labeled data enjoying the representation learned by the \std. This extends LUPI in a way that the \tch provides privileged information that is most useful for the current state of \std's knowledge. \fwl also extends LUPI by introducing several teachers each of which is specialized to correct \std's knowledge related to a specific region of the data space.

% Figure~\ref{fig:learning_rate}(a) provides evidence for the assumption that privileged information in our task can accelerate the learning process of the \std. It shows how the privileged information from an intelligent teacher affects the exponent $\alpha$ of the error bound in Equation~\ref{eq:LUPI_error_bound}. Figure~\ref{fig:learning_rate}(b) shows the test error for various number of samples $|\mathcal{D}_s|$ with true label. As expected, In both extremes where $|\mathcal{D}_s|$ is too small or too large, the performance of our model becomes close to the models without a teacher. The reason is that \std has enough strong samples to learn a good model of true function. In more realistic cases where $|\mathcal{D}_s|\ll|\mathcal{D}_w|$ but $|\mathcal{D}_s|$ is still large enough to be informative about $|\mathcal{D}_w|$, our model gives a lower test error than models without the intelligent teacher.

The theory of LUPI was first developed and proved for support vector machines by Vapnik as a method for knowledge transfer. Hinton introduced~\emph{Dark knowledge} as a spiritually close idea in the context of neural networks~\citep{Hinton:2006}. He proposed to use a large network or an ensemble of networks for training and a smaller network at test time. It turned out that compressing knowledge of a large system into a smaller system can improve the generalization ability. It was shown 
%in~\citep{lopez2015unifying} 
in~\citep{lopez:2015} that dark knowledge and LUPI can be unified under a single umbrella, called \emph{generalized distillation}. The core idea of these models is \emph{machines-teaching-machines}. As the name suggests, a machine is learning the knowledge embedded in another machine. In our case, \std is correcting his knowledge by receiving privileged information about label uncertainty from \tch. 

Our framework extends the core idea of LUPI in the following directions:
\begin{itemize}[leftmargin=*]%
%save some space
\setlength{\topsep}{0.1pt}
\setlength{\partopsep}{0.1pt}
\setlength{\itemsep}{0.1pt}
\setlength{\parskip}{0.1pt}
\setlength{\parsep}{0.1pt}
    \item Trainable teacher: It is often assumed that the teacher in LUPI framework has some additional true information. We show that when this extra information is not available, one can still use the LUPI setup and define an implicit teacher whose knowledge is learned from the true data. In this approach, the performance of the final \std-\tch system depends on a clever answer to the following question: which information should be considered as the privileged knowledge of \tch.
  \item Bayesian teacher: The proposed teacher is Bayesian. It provides posterior uncertainty of the label of each sample.
  \item Mutual representation: We introduced module $\psi(.)$ which learns a mutual embedding (representation) for both \std and teacher. This is in particular interesting because it defines a two-way channel between teacher and \std. 
  \item Multiple teachers: We proposed a scalable method to introduce several teachers such that each teacher is specialized in a particular region of the data space.
\end{itemize}