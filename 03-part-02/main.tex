% !TEX root = ../thesis-main.tex
\part{\titleof{p2-with-break}} % doesn't work!
\label{part2}
The unprecedented success of data-driven approaches has turned data into a first-class citizen in machine learning. Most of the times, the more data you have, the more accurate your model will be~\citep{halevy2009unreasonable,sun2017revisiting} and it is more crucial to provide massive amounts of training data when the models become more deep and complex.
Collecting such training sets by hand is often infeasible due to the time and expense of labeling data. Besides, hand-labeled training sets are static and we might need complete relabeling, for instance, when the modeling goals changes. Thus moving beyond fully supervised learning, like adapting weak supervision with the hope of overcoming the poverty of stimulus, is a key direction in machine learning research.

Humans learn effortlessly from weak and inconsistent signals. However, it seems difficult to build fault-tolerant machine learning systems that learn, while even imperfect signals can contain a great deal of valid information.
Given the fact that only a tiny portion of real word applications operate under perfect conditions, an essential aspect of any practical learning algorithm is the need to learn from inconsistent data provided by different sensors, noisy or weak supervision, and even when crucial information is missing from the supervision signal.

In Part~\ref{part2} of this thesis, we address the following research question:
\resq{p2}

The imperfect examples can come from labels provided by non-expert crowd workers, be the output of other models that are weaker (for instance, with low accuracy or coverage), biased, or models trained on data from different related domains. 
Here in this part, we aim to study how the human can supervise machine learning systems, by labeling training data programmatically instead of labeling by hand. Then, given a vast amount of pragmatically generated labeled data, and maybe a small set of examples with true labels,  we discuss how to design neural networks that leverage the full capacity of the information in the data and go beyond the imperfection in the weakly annotated data.


In the first chapter of this part, Chapter~\ref{chap:4}, we address the following research question:
\resq{c4}

In this chapter, we propose to train a neural ranking model using weak labels that are obtained automatically without human annotators or any external resources (e.g., click data). We train a set of simple yet effective neural ranking models and study their effectiveness under various learning scenarios, i.e., point-wise and pair-wise, different objective functions, and using different input representations, from using a set of engineered features to encoding query/document using word embedding~\citep{Dehghani:2017:SIGIR}. We also discuss how privacy preserving approaches can benefit from models that are capable of learning from weak signals, where instead of labels from the original sensitive training data a noisy version is provided~\citep{dehghani:2017:neuir}.

Then, in the second chapter of this part, Chapter~\ref{chap:5} we focus on the following research question:
\resq{c5}

In this chapter we introduce \emph{Learning with Controlled Weak Supervision (\cws)}~\cite{Dehghani:2017:nips_metalearn, Dehghani:2017avoiding} and \emph{Fidelity Weighted Learning (\fwl)}~\citep{dehghani:2018:ICLR,Dehghani:2019:ICLR-LLD}, two semi-supervised approaches for training neural networks, where we have a large set of data with weak labels and a small amount of data with true labels. 
%
In \cws we train two neural networks in a meta-learning setup: a \tnet, the learner and a \cnet, the meta-learner.  The \tnet is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to control the magnitude of the gradient updates to the \tnet using the scores provided by the second \cnet, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the \tnet model.
%
\fwl is a student-teacher approach in which we modulate the parameter updates to a \emph{student} network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a \emph{teacher} (who has access to the high-quality labels).  


We show that we can train a neural ranker using a heuristic labeling function as weak supervision signal and go beyond the performance of this weak annotator, merely by choosing right architecture and objective functions and discuss how this can benefit learning in a privacy-preserving setup.
Given a semi-supervised setup, we apply our introduced methods, \cws and \fwl, to a range of language understanding tasks and empirically verify that they improve over semi-supervised alternatives and speeds up the training process. 

\medskip

\input{03-part-02/chapter-04/main.tex}
\input{03-part-02/chapter-05/main.tex}