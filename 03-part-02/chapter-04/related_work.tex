\section{Related Work}
In this section, we briefly review the neural ranking models in terms of their general architectures and discuss how the neural models proposed in this chapter are related to the previous works.

Recently, several attempts have been made to study deep neural networks in IR applications, which can be generally partitioned into two categories~\citep{guo2019deep, Onal:2016, Zhang:2016}. 
The first category includes approaches that use the results of trained (deep) neural networks in order to improve the performance in IR applications. Among these, distributed word representations or embeddings~\citep{Mikolov:2013,Pennington:2014} have attracted a lot of attention. Word embedding vectors have been applied to term re-weighting in IR models~\citep{Zheng:2015}, query expansion~\citep{Diaz:2016,Zamani:2016a}, query classification~\citep{Liu:2015,Zamani:2016b},  etc. 
The main shortcoming of most of the approaches in this category is that the objective of the trained neural network differs from the objective of these tasks.  For instance, the word embedding vectors proposed in~\citep{Mikolov:2013,Pennington:2014} are trained based on term proximity in a large corpus, which is different from the objective in most IR tasks. \citet{Zamani:2017} recently proposed relevance-based word embedding models for learning word representations based on the objectives that matter for IR applications.

The second category, which the models proposed in this chapter belong to, consists of the approaches that design and train a (deep) neural network for a specific task, e.g., question answering~\citep{Cohen:2016,Yang:2016}, click models~\citep{Borisov:2016}.
A number of the approaches in this category have been proposed for ranking documents in response to a given query.
These approaches can be generally divided into two groups: \emph{late combination models} and \emph{early combination models} (or representation-focused and interaction-focused models according to~\citep{Guo:2016}). 
The late combination models, following the idea of Siamese networks~\citep{Bromley:1993}, independently learn a representation for each query and candidate document and then calculate the similarity between the two estimated representations via a similarity function. For example, \citet{Huang:2013} proposed DSSM, which is a feed forward neural network with a word hashing phase as the first layer to predict the click probability given a query string and a document title. 
The DSSM model was further improved by incorporating convolutional neural networks~\citep{Shen:2014}.

On the other hand, the early combination models are designed based on the interactions between the query and the candidate document as the input of network. 
For instance, DeepMatch~\citep{Lu:2013} maps each text to a sequence of terms and trains a feed-forward network for computing the matching score. 
The deep relevance matching model for ad-hoc retrieval~\citep{Guo:2016} is another example of an early combination model that feeds a neural network with the  histogram-based features representing interactions between the query and document. 
Early combining enables the model to have an opportunity to capture various interactions between query and document(s), while with late combination approach, the model has only the chance of isolated observation of input elements. Recently, Mitra et al.~\citep{Mitra:2016} proposed to simultaneously learn local and distributional representations, which are early and late combination models respectively,  to capture both exact term matching and semantic term matching.

Until now, all the proposed neural models for ranking are trained on either explicit relevance judgements or clickthrough logs. However, a massive amount of such training data is not always available. 

In this chapter, we propose to train neural ranking models using weak supervision, which is the most natural way to reuse the existing supervised learning models where the imperfect labels are treated as the ground truth.
The basic assumption is that we can cheaply obtain labels (that are of lower quality than human-provided labels) by expressing the prior knowledge we have about the task at hand by specifying a set of heuristics, adapting existing ground truth data for a different but related task (this is often referred to distant supervision\footnote{We do not distinguish between weak and distant supervision as the difference is subtle and both terms are often used interchangeably in the literature.}), extracting supervision signal from external knowledge-bases or ontologies, crowd-sourcing partial annotations that are cheaper to get, etc.
%
Weak supervision is a natural way to benefit from unsupervised data and it has been applied in NLP for various tasks including relation extraction~\citep{Bing:2015,Han:2016}, knowledge-base completion~\citep{Hoffmann:2011}, sentiment analysis~\citep{Severyn:2015:SemEval}, etc.  
There are also similar attempts in IR for automatically constructing test collections~\citep{Asadi:2011} and learning to rank using labeled features, i.e., features that an expert believes they are correlated with relevance~\citep{Diaz:2016:ictir}.
In this chapter, we make use of traditional IR models as the weak supervision signal to generate a large amount of training data and train effective neural ranking models that outperform the baseline methods by a significant margin.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



To circumvent the lack of human-labeled training examples, unsupervised learning methods aim to model the underlying data distribution, thus learning powerful feature representations of the input data, which can be helpful for building more accurate discriminative models especially when little or even no supervised data is available. A large group of unsupervised neural models seeks to exploit the implicit internal structure of the input data, which in turn requires customized formulation of the training objective (loss function), targeted network architectures and often non-trivial training setups. 


For example in NLP, various methods for learning distributed word representations, e.g., word2vec~\citep{Mikolov:2013}, GloVe~\citep{Pennington:2014}, and sentence representations, e.g., paragraph vectors~\citep{Le:2014} and skip-thought~\citep{Kiros:2015} have been shown very useful to pre-train word embeddings that are then used for other tasks such as sentence classification, sentiment analysis, etc. Other generative approaches such as language modeling in NLP, and, more recently, various flavors of auto-encoders~\citep{Baldi:2012} and generative adversarial networks~\citep{Goodfellow:2014} in computer vision have shown a promise in building more accurate models.


A lot of research has been done on the general problem of preserving the privacy of sensitive data in IR applications, where the question is how should we design effective IR systems without damaging users' privacy?  One of the solutions so far is to anonymize the data and try to hide the identity of users~\citep{Carpineto:2013, Zhang:2016}.  As an example, \citet{Zhang:2016} use a differential privacy approach for query log anonymization. However, there is no guarantee that the anonymized data will be as effective as the original data.

Modeling privacy in machine learning is a challenging problem and there has been much research in this area. Preserving the privacy of deep learning models is even more challenging, as there are more parameters to be safeguarded~\citep{Phan:2016}. 
Some work has studied the vulnerability of deep neural network as a service, where the interaction with the model is only via an input-output black box~\citep{Tramer:2016, Fredrikson:2015, Shokri:2016}.
Others have proposed approaches to protect privacy against an adversary with a full knowledge of the training mechanism and access to the model's parameters. For instance, \citet{Abadi:2016} propose a privacy preserving stochastic gradient descent algorithm offering a trade-off between utility and privacy. More recently, \citet{Papernot:2017} propose a semi-supervised method for transferring the knowledge for deep learning from private training data. They propose a setup for learning privacy-preserving student models by transferring knowledge from an ensemble of teachers trained on disjoint subsets of the data for which privacy guarantees are provided.