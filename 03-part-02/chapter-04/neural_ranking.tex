\section{Weakly Supervised Neural Rankers}
\label{sec:weakly_supervised_neural_rankers}
Despite the promising performance from neural networks on many language understanding tasks, ranking has remained a challenging problem. Besides the inherent difficulty of ``assessing relevance,'' the lack of availability of public large scaled datasets that consist query-document pairs annotated by relevance labels, makes it difficult to advance data hungry models for this task.

Therefore, it is essential to come up with solutions that let us train neural ranking models, where there is no labeled data it is only available at an extremely limited size. 
One the main idea to tackle this problem is to make use of weak human supervision or weakly labeled data, as it is much cheaper to collect or readily available at much larger scale. This section focuses on addressing the following questions:
\resq{c4.1}

We propose to pseudo-label a large set of unlabeled data using an unsupervised method and train a neural ranker using these ``weak'' or ``noisy'' labels. Given this setup, we examine various neural ranking models with different ranking architectures and objectives, i.e., point-wise and pair-wise, as well as different input representations, from encoding query-document pairs into dense\:/\:sparse vectors to learning query\:/\:document embedding representations. 

Our results have broad impact as the proposal to use unsupervised traditional methods as weak supervision signals and is applicable to a variety of IR tasks, such as filtering or classification, without the need for supervised data.  More generally, our approach unifies the classic IR models with currently emerging data-driven approaches in an elegant way.

\subsection{Pseudo-Labeling Unlabeled Data}
\label{sec:pseudo_labeling}
We use the idea of ``Pseudo-Labeling'' and propose to leverage a classic unsupervised IR model to annotate a large amount of unlabeled data and infer weak labels and use this signal to train supervised models as if we had the ground truth labels.
Since the data is generated programmatically, we can generate billions of training samples with almost no cost.\footnote{Weak supervision for training a ranker may refer to using click-through data. Here, we assume that no external information, e.g. search logs, is available.}

We focus on query-dependent ranking as a core IR task. To this aim, we take a well-performing existing unsupervised retrieval model, such as BM25. This model plays the role of ``pseudo-labeler'' in our learning scenario. In more detail, given a target collection and a large set of training queries (without relevance judgments), we make use of the pseudo-labeler to rank/score the documents for each query in the training query set. The goal is to train a ranking model given the scores/ranking generated by the pseudo-labeler as a weak supervision signal.

In the followings, we describe different neural architectures in details and finally investigate their effectiveness when trained on weakly annotated data.  

\subsection{Neural Ranking Architectures}
\label{sec:neural_ranking_arch}
In this section, we introduce three different neural ranking models that are trained based on different ``objectives''. We describe the architecture of the base neural network shared by these models. We further discuss the three different ``input layers'' used in our neural rankers to encode information of given query-document pairs.
\medskip

\label{sec:models}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.26\columnwidth}
        \centering
        \includegraphics[height=5cm]{03-part-02/chapter-04/figs_and_tables/fig_model_1.png}%
        \caption{\label{fig:m1}\mone model}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.40\columnwidth}
        \centering
        \includegraphics[height=5cm]{03-part-02/chapter-04/figs_and_tables/fig_model_2.png}%
        \caption{\label{fig:m2}\mtwo model}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.37\columnwidth}
        \centering
        \includegraphics[height=5cm]{03-part-02/chapter-04/figs_and_tables/fig_model_3.png}%
        \caption{\label{fig:m3}\mthree model}
    \end{subfigure}%
    \caption{\label{fig:ranking-arch} Different ranking architectures.}
\end{figure}
%

We define three different ranking models, one point-wise and two pair-wise models:

\subsubsection{\label{sec:modelone}\ModelOne} 
This architecture models a point-wise ranking model that learns to predict retrieval scores for query-document pairs. More formally, the goal in this architecture is to learn a \emph{scoring function} $\mathcal{S}(q, d; \theta)$ that determines the retrieval score of document $d$ for query $q$, given a set of model parameters $\theta$.
%
In the training stage, we are given a training set comprising of training samples each a triple $x = (q,d, s_{q,d})$, where $q$ is a query from training query set $Q$, $d$ represents a retrieved document for the query $q$, and $s_{q,d}$ is the relevance score (calculated by a weak supervisor), which is acquired using a retrieval scoring function in our setup.
%
We consider the mean squared error as the loss function for a given batch of training samples:
\begin{equation}
\mathcal{L}(b; \theta) = \frac{1}{|b|} \sum_{i=1}^{|b|}{(\mathcal{S}(\{q, d\}_i; \theta) - s_{\{q, d\}_i})^2}
\end{equation}
where $\{q, d\}_i$ denotes the query and the corresponding retrieved document in the $i^{th}$ training sample, i.e., $x_i$ in the batch $b$.
The conceptual architecture of the model is illustrated in Figure~\ref{fig:m1}.


\subsubsection{\label{sec:modeltwo}\ModelTwo}
In this model, similar to the previous one, the goal is to learn a scoring function $\mathcal{S}(q, d; \theta)$ for a given pair of query $q$ and document $d$ with the set of model parameters $\theta$. 
However, unlike the previous model, we do not aim to learn a calibrated scoring function. 
In this model, as is depicted in Figure~\ref{fig:m2}, we use a pair-wise scenario during training in which we have two point-wise networks that share parameters and we update their parameters to minimize a pair-wise loss.
Each training sample has five elements: $x = (q,d_1, d_2, s_{q,d_1}, s_{q,d_2})$.
During the inference, we treat the trained model as a point-wise scoring function to score query-document pairs.

We have tried different pair-wise loss functions and empirically found that the model learned based on the hinge loss (max-margin loss function) performs better than the others. 
Hinge loss is a linear loss that penalizes examples that violate the margin constraint. It is widely used in various learning to rank algorithms, such as Ranking SVM~\citep{Herbrich:1999}. The hinge loss function for a batch of training samples is defined as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}(b; \theta) = \frac{1}{|b|}
\sum_{i=1}^{|b|}
\max\big\{
& 
0, \varepsilon - \text{sign}
(s_{\{q, d_1\}_i} - s_{\{q, d_2\}_i})
& \\ & 
\left(\mathcal{S}\left(\{q, d_1\}_i; \theta\right) -\mathcal{S}\left(\{q, d_2\}_i; \theta\right)\right)
\big\}
, 
%\nonumber
\end{aligned}     
\end{equation}
where $\varepsilon$ is the parameter determining the margin of hinge loss. We found that as we compress the outputs to the range of $[-1, 1]$, $\varepsilon=1$ works well as the margin for the hinge loss function.

\subsubsection{\label{sec:modelthree}\ModelThree}
The third architecture is based on a pair-wise scenario during both training and inference (Figure~\ref{fig:m3}). This model learns a \emph{ranking function} $\mathcal{R}(q, d_1, d_2; \theta)$ which predicts the probability of document $d_1$ to be ranked higher than $d_2$ given $q$.
Similar to the \modeltwo, each training sample has five elements: $x = (q,d_1, d_2, s_{q,d_1}, s_{q,d_1})$.
For a given batch of training samples, we define our loss function based on cross-entropy as follows:
\begin{align}
\mathcal{L}(b; \theta) = -\frac{1}{|b|}
\sum_{i=1}^{|b|} &
P_{\{q,d_1,d_2\}_i} \log(\mathcal{R}(\{q,d_1,d_2\}_i; \theta)) \\
&
+ (1- P_{\{q,d_1,d_2\}_i})\log(1- \mathcal{R}(\{q,d_1,d_2\}_i; \theta)) \nonumber
\end{align}
where $P_{\{q,d_1,d_2\}_i}$ is the probability of document $d_1$ being ranked higher than $d_2$, based on the scores obtained from training sample $x_i$:
\begin{equation}
P_{\{q,d_1,d_2\}_i} = \frac{s_{\{q,d_1\}_i}}{s_{\{q,d_1\}_i} + s_{\{q,d_2\}_i}}
\end{equation}

A similar loss function has been previously used in RankNet~\citep{Burges:2005}. It is notable that at inference time, we need a scalar score for each document. Therefore, we need to turn the model's pair-wise predictions into a score per document. To do so, for each document, we calculate the average of predictions against all other candidate documents, which has $O(n^2)$ time complexity and is not practical in real-world applications. There are some approximations could be applicable to decrease the time complexity at inference time~\citep{Wauthier:2013}.

\medskip
As shown in Figure~\ref{fig:ranking-arch}, all the described ranking architectures share a neural network module. In all these models, we opted for a simple feed-forward neural network which is composed of: input layer $z_0$, $l-1$ hidden layers, and the output layer $z_l$. The input layer $z_0$ provides a mapping $\psi$ to encode the input query and document(s) into a fixed-length vector.
The exact specification of the input representation feature function $\psi$ is given in the next subsection. 
Each hidden layer $z_i$ is a fully-connected layer that computes the following transformation:
\begin{equation}
    z_i = \alpha(W_i.z_{i-1} + b_i); ~ 1<i<l-1,
\end{equation}
where $W_i$ and $b_i$ respectively denote the weight matrix and the bias term corresponding to the $i^{th}$ hidden layer, and $\alpha(.)$ is the activation function. We use the rectifier linear unit $\textit{ReLU}(x) = \max(0, x)$ as the activation function, which is a common choice in the deep learning literature~\citep{Lecun:2015}. 
The output layer $z_l$ is a fully-connected layer with a single continuous output. The activation function for the output layer depends on the ranking architecture that we use. For the \modelone architecture, we empirically found that a linear activation function works best, while $tanh$ and the sigmoid functions are used for the \modeltwo and \modelthree respectively.

Furthermore, to prevent feature co-adaptation, we use dropout as the regularization technique in all the models. Dropout sets a portion of hidden units to zero during the forward phase when computing the activations which prevents overfitting.


\subsection{Representing Inputs}
\label{sec:feedings}
We explore three definitions of the input layer representation $z_0$ captured by a feature function $\psi$ that maps the input into a fixed-size vector which is further fed into the fully connected layers: 
(i) a conventional dense feature vector representation that contains various statistics describing the input query-document pair, 
(ii) a sparse vector containing bag-of-words representation, and 
(iii) bag-of-embeddings averaged with learned weights. 
These input representations define how much capacity is given to the network to extract discriminative signal from the training data and thus result in different generalization behavior of the networks. 
It is noteworthy that input representation of the networks in the \modelone and \modeltwo is defined for a pair of the query and the document, while the network in the \modelthree needs to be fed by a triple of the query, the first document, and the second document.

\subsubsection{\Feedone (\fone)} 
In this setting, we build a dense feature vector composed of features used by traditional IR methods, e.g., BM25. The goal here is to let the network fit the function described by the BM25 formula when it receives exactly the same inputs. 
In more detail, our input vector is a concatenation ($||$) of the following inputs: total number of documents in the collection (i.e., $N$), average length of documents in the collection (i.e., $avg(l_d)_D$), document length (i.e., $l_d$), frequency of each query term $t_i$ in the document (i.e., $tf(t_i, d)$), and document frequency of each query term (i.e., $df(t_i)$). Therefore, for the point-wise setting, we have the following input vector:
\begin{equation}
\psi(q, d) = [N || avg(l_d)_D || l_d || \{df(t_i) || tf(t_i,d)\}_{1 \leq i \leq k}],
\end{equation}
where $k$ is set to a fixed value ($5$ in our experiments). 
We truncate longer queries and do zero padding for shorter queries. 
For the networks in the \modelthree, we consider a similar function with additional elements: the length of the second document and the frequency of query terms in the second document.


\subsubsection{\Feedtwo (\ftwo)} 
Next, we move away from a fully featurized representation that contains only aggregated statistics and let the network performs feature extraction for us. In particular, we build a bag-of-words representation by extracting term frequency vectors of query ($tfv_q$), document ($tfv_d$), and the collection ($tfv_c$) and feed the network with concatenation of these three vectors. For the point-wise setting, we have the following input vector:
\begin{equation}
\psi(q, d) = [tfv_c || tfv_q || tfv_d]
\end{equation}
For the network in \modelthree, we have a similar input vector with both $tfv_{d_1}$ and $tfv_{d_2}$. Hence, the size of the input layer is $3 \times vocab~size$ in the point-wise setting, and $4 \times vocab~size$ in the pair-wise setting. 
%In this paradigm, we have no document-level statistic, e.g., document frequency, although collection term frequency would be considered as a clue for term weighting. 

\subsubsection{\label{sec:feedthree}\Feedthree (\fthree)}
The major weakness of the previous input representation is that words are treated as discrete units, hence prohibiting the network from performing soft matching between semantically similar words in queries and documents. In this input representation paradigm, we rely on word embeddings to obtain more powerful representations of queries and documents that could bridge the lexical chasm. % of \fthree.
The representation function $\psi$ consists of three components: an embedding function $\mathcal{E}: \mathcal{V} \rightarrow \mathbb{R}^{m}$ (where $\mathcal{V}$ denotes the vocabulary set and $m$ is the embedding dimension), a weighting function $\mathcal{W}: \mathcal{V} \rightarrow \mathbb{R}$, and a compositionality function $\odot: (\mathbb{R}^{m}, \mathbb{R})^n \rightarrow \mathbb{R}^{m}$. More formally, the function $\psi$ for the point-wise setting is defined as:
\begin{equation}
\psi(q, d) = [\odot_{i=1}^{|q|}(\mathcal{E}(t_i^q), \mathcal{W}(t_i^q)) || \odot_{i=1}^{|d|} (\mathcal{E}(t_i^d), \mathcal{W}(t_i^d))],
\end{equation}
where $t_i^q$ and $t_i^d$ denote the $i^{th}$ term in query $q$ and document $d$, respectively. 
For the network of the \modelthree, another similar term is concatenated with the above vector for the second document. The embedding function $\mathcal{E}$ transforms each term to a dense $m$-dimensional float vector as its representation, which is learned during the training phase. The weighting function $\mathcal{W}$ assigns a weight to each term in the vocabulary set, which is supposed to learn term global importance for the retrieval task. The compositionality function $\odot$ projects a set of $n$ embedding and weighting pairs to an $m$-dimensional representation, independent from the value of $n$. The compositionality function is given by:
\begin{equation}
\odot_{i=1}^n(\mathcal{E}(t_i), \mathcal{W}(t_i)) = \sum_{i=1}^n \widehat{\mathcal{W}}(t_i)\cdot \mathcal{E}(t_i),
\end{equation}
which is the weighted element-wise sum of the terms' embedding vectors. $\widehat{\mathcal{W}}$ is the normalized weight that is learned for each term, given as follows:
\begin{equation}
\widehat{\mathcal{W}}(t_i) = \frac{\exp(\mathcal{W}(t_i))}{\sum_{j=1}^n{ \exp(\mathcal{W}(t_j))}}
\end{equation}

\medskip
All combinations of different ranking architectures and different input representations presented in this section can be considered for developing ranking models.

\subsection{Training Neural Rankers with Weak Supervision}
\label{sec:neuralranking_expermients}
In this section,  we discuss the effectiveness of our neural rankers with different learning objectives (Section~\ref{sec:models}) and different input representations (Section~\ref{sec:feedings}), when they are trained with weakly supervised signals.

In the following, we first describe the train and evaluation data, metrics we report, and detailed experimental setup. Then we discuss the results.

\input{03-part-02/chapter-04/figs_and_tables/table_data.tex}
\subsubsection{Collections}
\label{sec:collections}
In our experiments, we used two standard TREC collections: The first collection (called \emph{Robust04}) consists of over 500k news articles from different news agencies, that is available in TREC Disks 4 and 5 (excluding Congressional Records). This collection, which was used in TREC Robust Track 2004, is considered as a homogeneous collection, because of the nature and the quality of documents. The second collection (called \emph{ClueWeb}) that we used is ClueWeb09 Category B, a large-scale web collection with over 50 million English documents, which is considered as a heterogeneous collection. This collection has been used in TREC Web Track, for several years. In our experiments with this collection, we filtered out the spam documents using the Waterloo spam scorer\footnote{\url{http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/}}~\citep{Cormack:2011} with the default threshold $70\%$. The statistics of these collections are reported in Table~\ref{tab:data}. 

\subsubsection{Training query set}
\label{sec:query_set}
To train our neural ranking models, we used the unique queries (only the query string) appearing in the AOL query logs~\citep{Pass:2006}. This query set contains web queries initiated by real users in the AOL search engine that were sampled from a three-month period from March 1, 2006 to May 31, 2006. We filtered out a large volume of navigational queries containing URL substrings (``http'', ``www.'', ``.com'', ``.net'', ``.org'', ``.edu''). We also removed all non-alphanumeric characters from the queries. We made sure that no queries from the training set appear in our evaluation sets. For each dataset, we took queries that have at least ten hits in the target corpus using the pseudo-labeler method. Applying all these processes, we ended up with 6.15 million queries for the Robust04 dataset and 6.87 million queries for the ClueWeb dataset. 
In our experiments, we randomly selected $80\%$ of the training queries as training set and the remaining $20\%$ of the queries were chosen as the validation set for hyper-parameter tuning. As the ``pseudo-labeler'' in our training data, we have used BM25 to score/rank documents in the collections given the queries in the training query set.

\subsubsection{Evaluation query sets} 
We use the following query sets for evaluation that contain human-labeled judgments: a set of 250 queries (TREC topics 301--450 and 601--700) for the Robust04 collection that were previously used in TREC Robust Track 2004. A set of 200 queries (topics 1-200) were used for the experiments on the ClueWeb collection. These queries were used in TREC Web Track 2009--2012. We only used the title of topics as queries.

\subsubsection{Evaluation Metrics}
To evaluate retrieval effectiveness, we report three standard evaluation metrics: mean average precision (MAP) of the top-ranked $1,000$ documents, precision of the top-$20$ retrieved documents (P@20), and normalized discounted cumulative gain (nDCG)~\citep{Jarvelin:2002} calculated for the top-$20$ retrieved documents (nDCG@20). Statistically significant differences of MAP, P@20, and nDCG@20 values are determined using the two-tailed paired t-test with $p\_value<0.05$, with Bonferroni correction.


\subsubsection{Experimental Setup}
All models described in Section~\ref{sec:models} are implemented using TensorFlow~\citep{tang2016:tflearn,tensorflow2015-whitepaper}.
In all experiments, the parameters of the network are optimized employing the Adam optimizer~\citep{Kingma:2014} and using the computed gradient of the loss to perform the back-propagation algorithm.
All model hyper-parameters were tuned on the respective validation set using batched GP bandits with an expected improvement acquisition function~\citep{Desautels:2014}. 
For each model, the size of hidden layers and the number of hidden layers were selected from $[16, 32, 64, 128, 256, 512, 1024]$ and $[1, 2, 3, 4]$, respectively. The initial learning rate and the dropout parameter were selected from $[1\times 10^{-3}, 5\times 10^{-4}, 1\times 10^{-4}, 5\times 10^{-5}, 1\times 10^{-5}]$ and $[0.0, 0.1, 0.2, 0.5]$, respectively. For models with \feedthree, we considered embedding sizes of $[100, 300, 500, 1000]$. As the training data, we take the top-$1,000$ retrieved documents for each query from training query set $Q$, to prepare the training data. In total, we have $|Q|\times 1000$ ($\sim6\times 10^{10}$ examples in our data) point-wise example and $\sim|Q|\times 1000^2$ ($\sim6\times 10^{13}$ examples in our data) pair-wise examples. The batch size in our experiments was selected from  $[128, 256, 512]$.
%
At inference time, for each query, we take the top-$2,000$ retrieved documents using BM25 as candidate documents and re-rank them by the trained models. In our experiments, we use the Indri\footnote{\url{https://www.lemurproject.org/indri.php}} implementation of BM25 with the default parameters (i.e., $k_1 = 1.2$, $b = 0.75$, and $k_3 = 1000$).


\bigskip
Given the setup we explained above, we train and evaluate our neural rankers to address our second research question in this chapter:
\resq{c4.2}

We attempt to break down our experiments and analyses to different parts addressing several subquestions, and provide empirical answers along with the intuition and analysis behind each question:

\input{03-part-02/chapter-04/figs_and_tables/table_main_results.tex} 
\subsubsection{How do the neural models with different training objectives and input representations compare?}
%
Table~\ref{tbl_main} presents the performance of all model combinations.
Interestingly, combinations of the \modeltwo and the \modelthree with \feedthree outperform BM25 by significant margins in both collections. For instance, the \modelthree with \feedthree that shows the best performance among the other methods, surprisingly, improves BM25 by over $13\%$ and $35\%$ in Robust04 and ClueWeb collections respectively, in terms of MAP. Similar improvements can be observed for the other evaluation metrics.

Regarding the modeling architecture, in the \modeltwo and the \modelthree, compared to the \modelone, we define objective functions that target to learn ranking instead of scoring. This is particularly important in weak supervision, as the scores are imperfect values---using the ranking objective alleviates this issue by forcing the model to learn a preference function rather than reproduce absolute scores.
%
In other words, using the ranking objective instead of learning to predict calibrated scores allows the \modeltwo and the \modelthree to learn to distinguish between examples whose scores are close. This way, some small amount of noise, which is a common problem in weak supervision, would not perturb the ranking as easily~\citep{Zamani:2018:ictir}.

Regarding the input representations, \feedthree leads to better performance compared to the other ones in all models.
Using \feedthree not only provides the network with more information, but also lets the network to learn proper representation capturing the needed elements for the next layers with a better understanding of the interactions between query and documents. 
Providing the network with already engineered features would block it from going beyond the weak supervision signal and limit the ability of the models to learn latent features that are unattainable through feature engineering. 

Note that although the \modelthree is more precise in terms of MAP, the \modeltwo is much faster in the inference time ($O(n)$ compared to $O(n^2)$), which is a desirable property in real-life applications.

\input{03-part-02/chapter-04/figs_and_tables/plot_loss_step.tex}
\subsubsection{Why do \feedone and \feedtwo fail to replicate the performance of BM25?}
%
Although neural networks are capable of approximating arbitrarily complex non-linear functions, we observe that the models with \feedone fail to replicate the BM25 performance, while they are given the same feature inputs as the BM25 components (e.g., TF, IDF, average document length, etc). To ensure that the training converges and there is no overfitting, we have looked into the training and validation loss values of different models during the training time. Figure~\ref{fig:step-loss} illustrates the loss curves for the training and validation sets per training step for different models.
%
As shown, in models with \feedone, the training losses drop quickly to values close to zero while this is not the case for the validation losses, which is an indicator of over-fitting on the training data. 
Although we have tried different regularization techniques, like $l_2$-regularization and dropout with various parameters, there is less chance for generalization when the networks are fed with the fully featurized input. Note that over-fitting would lead to poor performance, especially in weak supervision scenarios as the network learns to model imperfections from weak annotations. 
%
This phenomenon is also the case for models with the \feedtwo, but with less impact. However, in the models with the \feedthree, the networks do not overfit, which helps it to go beyond the weak supervision signals in the training data.

\input{03-part-02/chapter-04/figs_and_tables/plot_models_proximity.tex}
\subsubsection{How are the models related?}
%
To better understand the relationship of different neural models described above, we compare their performance across the query dimension following the approach in~\citep{Mitra:2016}. 
We assume that similar models should perform similarly for the same queries. Hence, we represent each model by a vector, called the performance vector, whose elements correspond to per query performance of the model, in terms of nDCG@20. The closer the performance vectors are, the more similar the models are in terms of the query by query performance. For the sake of visualization, we reduce the vectors dimension by projecting them to a two-dimensional space, using t-Distributed Stochastic Neighbor Embedding (t-SNE)\footnote{\url{https://lvdmaaten.github.io/tsne/}}.

Figure~\ref{fig:modelproximity} illustrates the proximity of different models in the Robust04 collection. Based on this plot, models with similar input representations (same color) have quite close performance vectors, which means that they perform similarly for the same queries. This is not necessarily the case for models with similar architecture (same shape). 
This suggests that the amount and the way that we provide information to the networks are the key factors in the ranking performance. 

We also observe that the \modelone with \feedone is the closest to BM25 which is expected. 
It is also interesting that models with \feedthree are placed far away from other models which shows they perform differently compared to the other input representations.


\subsubsection{How meaningful are the compositionality weights learned in the \feedthree?}
%
In this experiment, we focus on the best performing combination, i.e., the \modelthree with \feedthree. To analyze what the network learns, we look into the weights $\mathcal{W}$ (see Section~\ref{sec:feedthree}) learned by the network. Note that the weighting function $\mathcal{W}$ learns a global weight for each vocabulary term. We notice that in both collections there is a strong linear correlation between the learned weights and the inverse document
%
\begin{figure}[!t]%
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{03-part-02/chapter-04/figs_and_tables/plot_composionality_idf_scatter_robust.png}
        \caption{\label{fig:scatter_r}Robust04}{\scriptsize{(Pearson Correlation: 0.8243)}}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{03-part-02/chapter-04/figs_and_tables/plot_composionality_idf_scatter_clueweb.png}
        \caption{\label{fig:scatter_c}ClueWeb}{\scriptsize{(Pearson Correlation: 0.7014)}}
    \end{subfigure}%
    \caption{\label{fig:scatter}Strong linear correlation between weight learned by the compositionality function in the \feedthree and inverse document frequency.}
\end{figure}
%
Figure~\ref{fig:scatter} illustrates the scatter plots of the learned weight for each vocabulary term and its IDF, in both collections.
This is an interesting observation as we do not provide any global corpus information to the network in training and the network is able to infer such global information by only observing individual training samples.
% This demonstrates the ability of neural networks to automatically extract meaningful features for the task.

\input{03-part-02/chapter-04/figs_and_tables/table_diff_embeddings_results.tex}
\subsubsection{How well do other alternatives for the embedding and weighting functions in the \feedthree perform?}
Considering \feedthree as the input representation, we have examined different alternatives for the embedding function $\mathcal{E}$: (1) employing pre-trained word embeddings learned from an external corpus (we used Google News), (2) employing pre-trained word embeddings learned from the target corpus (using the skip-gram model \cite{Mikolov:2013}), and (3) learning embeddings during the network training as explained in Section~\ref{sec:feedthree}. 
Furthermore, for the compositionality function $\odot$, we tried different alternatives: (1) uniform weighting (simple averaging which is a common approach in compositionality function), (2) using IDF as fixed weights instead of learning the weighting function $\mathcal{W}$, and (3) learning weights during the training as described in Section~\ref{sec:feedthree}.

Table~\ref{tbl_res_m3f3_em} presents the performance of all these combinations on both collections. 
We note that learning both embedding and weighting functions leads to the highest performance in both collections. These improvements are statistically significant.
%
According to the results, regardless of the weighting approach, learning embeddings during training outperforms the models with fixed pre-trained embeddings.
%
This supports the hypothesis that with the \feedthree the neural networks learn an embedding that is based on the interactions of query and documents that tends to be tuned better to the corresponding ranking task.
%
Also, regardless of the embedding method, learning weights helps models to get better performance compared to the fixed weightings, with either IDF or uniform weights. 
%
Although weight learning can significantly affect the performance, it has less impact than learning embeddings.

Note that in the models with pre-trained word embeddings, employing word embeddings trained on the target collection outperforms those trained on the external corpus in the ClueWeb collection; while this is not the case for the Robust04 collection. The reason could be related to the collection size, since the ClueWeb is approximately $100$ times larger than the Robust04.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{03-part-02/chapter-04/figs_and_tables/plot_with_pretrained_emb_robust.png}
        \caption{\label{fig:embedding_r}Robust04}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{03-part-02/chapter-04/figs_and_tables/plot_with_pretrained_emb_clueweb.png}
        \caption{\label{fig:embedding_c}ClueWeb}
    \end{subfigure}%
    \caption{\label{fig:embedding}Performance of the \modelthree with learned embedding, pre-trained embedding, and learned embedding with pre-trained embedding as initialization, with respect to different amount of training data.}
\end{figure}

In addition to the aforementioned experiments, we have also tried initializing the embedding matrix with a pre-trained word embedding trained on the Google News corpus, instead of random initialization.
%
Figure~\ref{fig:embedding} presents the learning curve of the models. According to this figure, the model initialized by a pre-trained embedding performs better than random initialization when a limited amount of training data is available. 
%
When enough training data is fed to the network, initializing with pre-trained embedding and random values converge to the same performance.
An interesting observation here is that in both collections, these two initializations converge when the models exceed the performance of the weak supervision source, which is BM25 in our experiments. 
This suggests that the convergence occurs when accurate representations are learned by the networks, regardless of the initialization.

%In other words, after the network sees enough amount of train data to go beyond the supervision signal, it does not matter that with which initialization the model has started to train.
\input{03-part-02/chapter-04/figs_and_tables/table_svm_results.tex}
\subsubsection{Are deep neural networks a good choice for learning to rank with weak supervision?}
%
To see if there is a real benefit from using a non-linear neural network in different settings, we examined RankSVM~\citep{Joachims:2002} as a strong-performing pair-wise learning to rank method with the linear kernel that is fed with different inputs: \feedone, \feedtwo, and \feedthree. Considering that off-the-shelf RankSVM is not able to learn embedding representations during training, for \feedthree, instead of learning embeddings we use a pre-trained embedding matrix trained on Google News and fixed IDF weights. 

The results are reported in Table~\ref{tbl_svm}. As BM25 is not a linear function, RankSVM with the linear kernel is not able to completely approximate it. However, surprisingly, for both \feedone and \feedtwo, RankSVM works as well as neural networks (see Table~\ref{tbl_main}). 
%
Also, compared to the corresponding experiment in Table~\ref{tbl_res_m3f3_em}, the performance of the neural network with an external pre-trained embedding and IDF weighting is not considerably better than RankSVM. 
This shows that having non-linearity in neural networks does not help that much when we do not have representation learning as part of the model.
%
Note that all of these results are still lower than BM25, which shows that they are not good at learning from weak supervision signals for ranking. 
%

We have also examined the \modelone with a network with a single linear hidden layer, with the \feedthree, which is equivalent to a linear regression model with the ability of representation learning. 
Comparing the results of this experiment with \mone-\fthree in Table~\ref{tbl_main}, we can see that with a single-linear network we are not able to achieve a performance that is as good as a deep neural network with non-linearity.
%
This shows that the most important superiority of deep neural networks over other machine learning methods is their ability to learn an effective representation and take all the interactions between query and document(s) into consideration for approximating an effective ranking/scoring function. 
This can be achieved when we have a deep enough network with non-linear activations.

%\alexi{Mosi@: we should also include supervised learning experiments on smaller subsets of human judgments, if possible. The idea is that in these scenarios where labeled data is scarce, weak supervision is really useful. Having plots, where we use various percentages of labeled examples and observing how weak supervision is increasingly important as the number of labeled examples becomes smaller.}
\input{03-part-02/chapter-04/figs_and_tables/table_semisup_results.tex}
\subsubsection{How useful is learning with weak supervision for supervised ranking?}
%
In this set of experiments, we investigate whether employing weak supervision as a pre-training step helps to improve the performance of supervised ranking, when a small amount of training data is available. Table~\ref{tbl_semisup} shows the performance of the \modelthree with the \feedthree in three situations: (1) when it is only trained on weakly supervised data (similar to the previous experiments), (2) when it is only trained on supervised data, i.e., relevance judgments, and (3) when the parameters of the network is pre-trained using the weakly supervised data and then fine-tuned using relevance judgments.
%
In all the supervised scenarios, we performed 5-fold cross-validation over the queries of each collection and in each step, we used the TREC relevance judgments of the training set as a supervised signal. For each query with $m$ relevant documents, we also randomly sampled $m$ non-relevant documents as negative samples. Binary labels are used in the experiments: $1$ for relevant documents and $0$ for non-relevant ones.

The results in Table~\ref{tbl_semisup} suggest that pre-training the network with a weak supervision signal, significantly improves the performance of supervised ranking.
%
The reason for the poor performance of the supervised model compared to the conventional learning to rank models is that the number of parameters is much larger, hence it needs much more data for training.

In situations when little supervised data is available, it is especially helpful to use unsupervised pre-training which acts as a network pre-conditioning that puts the parameter values in the appropriate range that renders the optimization process more effective for further supervised training~\citep{Rrhan:2010}.

With this experiment, we indicate that the idea of learning from weak supervision signals for neural ranking models, which is presented in this section, not only enables us to learn neural ranking models when no supervised signal is available, but also has substantial positive effects on the supervised ranking models with limited amount of training data. 

In the next section, we study how learning from weak/noisy label can benefit preserving privacy in machine learning, like in situations where additional noise is intentionally added to the training data to guarantee differential privacy.  