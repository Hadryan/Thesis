\section{Related Work}
This section discusses briefly the separation property in the related domains and review principles in information retrieval and text classification, which are associated with the concept of separability.  
In addition, some research on classification and topic modeling of hierarchical texts are discussed. 

Separability is a property which makes the data representation sufficient to distinguish instances and consequently enables autonomous systems to easily interpreter the data~\citep{Lewis:1992}. For instance, in the classification task, classifiers learn more accurate data boundaries when they are provided with separable representations of data from different classes~\citep{Lewis:1995}. The importance of separability in classifiers has led to the fact that making data separable becomes part of the classification. As the most familiar instances, SVM by adding extra dimensions implicitly transform the data into a new space where they are linearly separable~\citep{Burges:1998}.

Separation is also a pivoting concept in the information retrieval. Separating relevant from non-relevant documents is a fundamental issue in this domain~\citep{Robertson:1977,saracevic:1975,Lavrenko:2001}. In IR, separation plays a more important role when instead of giving a rank list, a decision should be made about the relevancy of documents, for example in the information filtering task~\citep{Lewis:1992}. As another instance, in the task of relevance feedback, there are some efforts on estimating a distinctive model for relevant documents so that it reflects not only  their similarity, but also their difference from the whole collection, i.e., what makes them stand out or separated~\citep{Sparck:2003,Hiemstra:2004,Zhai:SMM:2001}. 

In this chapter, we address the separation property in the textual data that are organizable in a hierarchical structure. In a hierarchy, due to the existence of dependencies between entities, estimating separated models is a complex task. There is a range of work on the problem of hierarchical text classification~\citep{Sebastiani:2002, Sun:2001}, which tried to model hierarchical text-based entities. \citet{McCallum:1998} proposed a method for modeling an entity in the hierarchy which tackles the problem of data sparseness in lower layer entities. They used a shrinkage estimator to smooth the model of each leaf entity with the model of its ancestors to make the models more reliable. 
There is also similar research on XML data processing, as hierarchically structured data, which try to incorporate evidence from other layers as the context through mixing each element language models by its parent's models~\citep{sigurbjornsson:2004,ogilvie:2004}.
%Their method, in fact, balances a trade-off between specificity and reliability. Although estimates in the leaf entities are more specific, due to data sparseness in these entities, the estimations are less reliable. Further up the hierarchy, there are more data instances so estimates are more reliable but unspecific.
%
%Later, \citet{Oh:2011}, similar to \citeauthor{McCallum:1998} tried to involve global information beside the local entity data to estimate the model of the entity. However, concerning large scale hierarchies, their method dynamically controls the level of information that is needed to be gathered from ancestors.

Recently, \citet{Song:2014} tackled the problem of representing hierarchical entities with a lack of training data for the task of hierarchical classification.  In their work, given a collection of instances and a set of hierarchical labels, they tried to embed all entities in a semantic space, then they construct a semantic representation for them to be able to compute meaningful semantic similarity between them.
%
%By focusing on scalability, \citet{Gopal:2013}, presented a framework which uses regularization technique for classifying entities with hierarchical and graphical dependencies to handle training data sparseness. They considered that the nearby entities in the hierarchy share similar model parameters and recursively push the parameters of the model of a node to be similar to its parents. This way, they encourage models of siblings to be similar to each other. 
%Again, their goal is to leverage information across the hierarchy that could be beneficial in case of training data sparseness. 

\citet{Zhou:2011} proposed a method that directly tackles the difficulty of modeling similar entities at lower levels of the hierarchy. They used regularization so that the model of lower level entities have the same general properties as their ancestors, in addition to some more specific properties. 
%
Although these methods tried to model hierarchical texts, their concerns were not making the models separable. Instead, they mostly addressed the problem of \emph{training data sparseness} \cite{Ha-Thuc:2011,Song:2014,McCallum:1998} or presenting techniques for \emph{handling large scale data} \cite{Gopal:2013,Oh:2011,Xue:2008,Ha-Thuc:2011}.

In terms of modeling hierarchical entities, \citet{Kim:2013} used Hierarchical Dirichlet Process \citep{Teh:2006} to construct models for entities in the hierarchies using their own models as well as the models of their ancestors.  Also, \citet{Zavitsanos:2011} used HDP to construct the model of entities in a hierarchy employing the models of its descendants. This research tries to bring out precise topic models using the structure of the hierarchy, but they do not aim to estimate separable models.  

As we discussed in Section~\ref{subsec:Separability}, our proposed approach can be employed as a feature selection method for text classification. Prior research on feature selection for textual information~\citep{SIGIR-Workshop-2010,Forman:2003} tried to improve classification accuracy or computational efficiency, while our method aims to provide a separable representation of data that helps train a transferable model. 
Apart from considering the hierarchical structure, our goals also differ from prior research on the transferability of models. For instance, research on constructing dynamic models for data streams~\citep{Yao:2009,Blei:2006} first discovered the topics from data and then tried to efficiently update the models as data changes over the time, while our method aims to identify tiny precise models that are more robust and remain valid over time.  Research on domain adaptation~\citep{Xue:2008:plsa,Chen:2011} also tried to tackle the problem of missing features when very different vocabulary is used in test and training data.  This differs from our approach considering the hierarchical relations, as we aim to estimate separable models that are robust against changes in the structure of entities relations, rather than changes in the corpus vocabulary.