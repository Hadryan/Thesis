\section{Related Work}
This section briefly discusses the separation property in the related domains and reviews principles in information retrieval and text classification that are associated with the concept of separability.  
In addition, some research on classification and topic modeling of hierarchical texts is discussed. 

Separability is a property that makes the data representation sufficiently rich to distinguish instances and consequently enables autonomous systems to easily interpret the data~\citep{Lewis:1992}. For instance, in the classification task, classifiers learn more accurate data boundaries when they are provided with separable representations of data from different classes~\citep{Lewis:1995}. The importance of separability in classifiers has led to the fact that making data separable becomes part of the classification. As the most familiar instances, SVM transform the data into a new space where they are linearly separable~\citep{Burges:1998}.

Separation is also a pivoting concept in information retrieval (IR). Separating relevant from non-relevant documents is a fundamental issue in this domain~\citep{Robertson:1977,saracevic:1975,Lavrenko:2001}. In IR, separation plays a more important role when instead of giving a ranked list, a decision should be made about the relevancy of documents, for example in the information filtering task~\citep{Lewis:1992}. As another instance, in the task of relevance feedback, there are some efforts on estimating a distinctive representation for relevant documents so that it reflects not only  their similarity, but also their difference from the whole collection, i.e., what makes them stand out or separated~\citep{Sparck:2003,Hiemstra:2004,Zhai:SMM:2001}. 

In this chapter, we address the separation property in textual data that is organizable in a hierarchical structure. In a hierarchy, due to the existence of dependencies between entities, estimating separable representations is a complex task. There is a range of work on the problem of hierarchical text classification~\citep{Sebastiani:2002, Sun:2001}, which tried to model hierarchical text-based entities. \citet{McCallum:1998} proposed a method for learning representation for an entity in the hierarchy which tackles the problem of data sparseness in lower layer entities. They used a shrinkage estimator to smooth the representation of each leaf entity with the representation of its ancestors to make them more reliable. 
There is also similar research on XML data processing, as hierarchically structured data, which tries to incorporate evidence from other layers as the context through mixing each element language models by its parent's models~\citep{sigurbjornsson:2004,ogilvie:2004}.
%Their method, in fact, balances a trade-off between specificity and reliability. Although estimates in the leaf entities are more specific, due to data sparseness in these entities, the estimations are less reliable. Further up the hierarchy, there are more data instances so estimates are more reliable but unspecific.
%
%Later, \citet{Oh:2011}, similar to \citeauthor{McCallum:1998} tried to involve global information beside the local entity data to estimate the model of the entity. However, concerning large scale hierarchies, their method dynamically controls the level of information that is needed to be gathered from ancestors.

Recently, \citet{Song:2014} tackled the problem of representing hierarchical entities with a lack of training data for the task of hierarchical classification.  In their work, given a collection of instances and a set of hierarchical labels, they tried to embed all entities in a semantic space, then they construct a semantic representation for them to be able to compute meaningful semantic similarity between them.
%
%By focusing on scalability, \citet{Gopal:2013}, presented a framework which uses regularization technique for classifying entities with hierarchical and graphical dependencies to handle training data sparseness. They considered that the nearby entities in the hierarchy share similar model parameters and recursively push the parameters of the model of a node to be similar to its parents. This way, they encourage models of siblings to be similar to each other. 
%Again, their goal is to leverage information across the hierarchy that could be beneficial in case of training data sparseness. 

\citet{Zhou:2011} proposed a method that directly tackles the difficulty of modeling similar entities at lower levels of the hierarchy. They used regularization so that the representation of lower level entities have the same general properties as their ancestors, in addition to some more specific properties. 
%
Although these methods tried to learn representation for hierarchical texts, their concerns were not making the representations separable. Instead, they mostly addressed the problem of training data sparseness~\citep{Ha-Thuc:2011,Song:2014,McCallum:1998} or presented techniques for handling large scale data~\citep{Gopal:2013,Oh:2011,Xue:2008,Ha-Thuc:2011}.

In terms of modeling hierarchical entities, \citet{Kim:2013} used Hierarchical Dirichlet Processes (HDPs)~\citep{Teh:2006} to learn representations for entities in the hierarchies using their own representations as well as representations of their ancestors.  Also, \citet{Zavitsanos:2011} used HDPs to learn a representation for entities in a hierarchy employing representations of their descendants. This research tries to bring out precise topic models using the structure of the hierarchy, but they do not aim to estimate separable representations.  

As we discussed in Section~\ref{subsec:Separability}, our proposed approach can be employed as a feature selection method for text classification. Prior research on feature selection for textual information~\citep{SIGIR-Workshop-2010,Forman:2003} tried to improve classification accuracy or computational efficiency, while our method aims to provide a separable representation of data that helps train a transferable model. 
Apart from considering the hierarchical structure, our goals also differ from prior research on the transferability of models. For instance, research on constructing dynamic models for data streams~\citep{Yao:2009,Blei:2006} first discovered the topics from data and then tried to efficiently update the models as data changes over the time, while our method aims to identify tiny precise representations that are more robust and remain valid over time.  Research on domain adaptation~\citep{Xue:2008:plsa,Chen:2011} also tried to tackle the problem of missing features when very different vocabularies are used in test and training data.  This differs from our approach considering the hierarchical relations, as we aim to estimate separable representations that are robust against changes in the structure of entities relations, rather than changes in the corpus vocabulary.