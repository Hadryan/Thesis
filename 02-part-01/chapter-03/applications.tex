\section{\achswlm for Hierarchical Classification}
Two-dimensional separability as the main property of \achswlm makes the learned representations of the entities in the hierarchy less sensitive to the structural changes, for instance, when the hierarchy evolves over time. In this section, we address our last research question of this chapter:
\resq{c3.3}

Before evaluating the transferability of two-dimensionally separable representations over time, we intrinsically study the separability of the representations learned by \achswlm. We use parliamentary data as one of the interesting collections with hierarchically structured data that can evolve over time. The structure of the parliamentary hierarchy has been shown in Figure~\ref{fig:ParHierarchy}.  First, we introduce the collection we have used, and then we analyze the quality of \achswlm on providing horizontal and vertical separability over the hierarchy. 

\subsection{Data Collection}
We use the Dutch parliamentary data which forms a hierarchical structure that can evolve over time. The data are collected and annotated as part of \emph{Political Mashup} project \citep{url:politicalmashup} to make semantically enriched parliamentary proceedings available as open data \citep{marx:2010}.

As a brief background, the Dutch parliament is a bicameral parliament that consists of a Senate and a House of Representatives. The House of Representatives is the main chamber of parliament, where discussion of proposed legislation and review of the government's actions takes place.  The Dutch parliamentary system is a multi-party system, requiring a coalition of parties to form the government~\citep{deswaan73}.

\input{02-part-01/chapter-03/figs_and_tables/fig_ducth_parl_hierarchy.tex}
We use data from the parliament or the House of Representatives of the Netherlands.  We have chosen three interesting periods of parliament, from March 2006 to April 2014, in which eight main parties have about 95\% of seats in the parliament: People's Party for Freedom and Democracy, Labour Party, Christian Democratic Appeal, Party for Freedom, The Socialist Party, Democrats 66, Green-Left, and Christian-Union.   The coalition in the first period is between a left-wing party and a centrist party, in the second period between a right-wing party and centrist party, and in the third, between a right-wing and left-wing party. Figure~\ref{fig:DutchParl} shows the hierarchical structure of the Dutch parliament in these three different periods.

In order to learn representations for parliamentary entities, first of all,  we prepare the data. In the proceedings, there are series of parliamentary speeches by different MPs following the debate structure.  We invert the data matrix so that for each speaker we collect their speeches as a single document that reflects the features of that member. Then, for representing the internal entities in the parliament's hierarchy, we first consider members as the leaf entities and then concatenate all leaf documents below internal entities as a single document which textually represents them: first over parties, and then parties into government and opposition, etc.
%
The whole corpus consists of 14.7 million terms from 240,501 speeches and contains 2.1 million unique terms. 
No stemming and no lemmatization is done on the data and also stop words, and common words are not removed in data preprocessing.  After data preparation, we estimate \acswlms for all entities in the hierarchy as it is explained in Section~\ref{sec:hswlm}.

%---------------------------------------
\subsection{Two-Dimensional Separability of \achswlm}
\label{subsec:achswlmep}
%---------------------------------------
Here we investigate the ability of \achswlm to provide language models for hierarchical entities that are two-dimensionally separable. 
Based on the explained procedure of estimating \achswlm, the language models of entities in the hierarchy are repeatedly updated, so that the resulting in representations are both \emph{horizontally} and \emph{vertically} separable in the hierarchy. To assess this fact, we estimate \achswlm on the parliamentary data and look into the separability between entities in the same layer or in different layers.

Figures~\ref{fig:HSS} and~\ref{fig:HSP} illustrate the probability distributions over terms based on the estimated \achswlms in the status and party layer, respectively. We sort the probability distribution based on the term weight of the first representation and plot the other representations in this exact order.
As can be seen in the status layer, Figure~\ref{fig:HSS}, the distributions over terms for government and opposition cover almost separated set of terms. Since in this layer these two entities are supposed to be against each other, a high level of separability can be expected. On the other hand, in the party layer, Figure~\ref{fig:HSP}, it is possible that two parties share some ideological issues and consequently share some terms. So, in this layer, a complete separability of terms would not be practically possible for all the parties. Nevertheless, \achswlm, to some extent, provides horizontal separability in this layer.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
\includegraphics[width=\linewidth]{02-part-01/chapter-03/figs_and_tables/img_opo-coa.png}
\caption{\label{fig:HSS}\achswlm in the status layer.}
        \end{subfigure}
        ~ 
    \begin{subfigure}[b]{0.64\textwidth}
\includegraphics[width=\linewidth]{02-part-01/chapter-03/figs_and_tables/img_parties.png}
\caption{\label{fig:HSP}\achswlm in the party layer.}
        \end{subfigure}
    \caption{\label{fig:HS} \emph{Horizontal Separability}: probability distribution over terms based on \hswlms in status layer and party layer.}
\end{figure}

Besides, we illustrate the horizontal separability of \achswlm of some pairs of parties. Figures \ref{fig:HSPCO}, \ref{fig:HSPOO}, and \ref{fig:HSPCC} show the separability of representations of two parties in three cases, respectively: 1) different statuses, 2) both in the status of opposition, 3) both in the status of government. It can be seen that in all cases of being in the same status or different status the estimated \hswlms are separable. The interesting point is in Figure~\ref{fig:HSPCC}, which the presents the representations of two government parties that are strongly separable. This is rooted in the fact that in this period there was an unusual coalition government consisting of a right-wing and a left-wing party. So, although they have an agreement in the status layer, their representation is highly separable in terms of having opposite spectrums in party layer.


\begin{figure}[!t]
        \centering
        \begin{subfigure}[b]{0.32\textwidth}
\includegraphics[width=\linewidth]{02-part-01/chapter-03/figs_and_tables/img_PvdA-CDA.png}
\caption{\label{fig:HSPCO} \achswlm of two parties in different statuses:CDA and PvdA.}
        \end{subfigure}
        ~ 
        \begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\linewidth]{02-part-01/chapter-03/figs_and_tables/img_PVV-CDA.png}
\caption{\label{fig:HSPOO} \achswlm of two parties in opposition: PVV and CDA.}
        \end{subfigure}
        ~ 
        \begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\linewidth]{02-part-01/chapter-03/figs_and_tables/img_VVD-PvdA.png}
\caption{\label{fig:HSPCC} \achswlm of two parties in government: VVD and PvdA.}
        \end{subfigure}
        \caption{\label{fig:HSP-pairs} \emph{Horizontal Separability}: probability distribution over terms based on \hswlms in party layer.}
\end{figure}

In order to illustrate the vertical separability of \achswlm, we choose two different branches in the hierarchy: one from the leader of one of the opposition parties to the root, and the other from the leader of one of the government parties to the root. Figure \ref{fig:VSO} and \ref{fig:VSC} show probability distributions over words based on \achswlm of all entities in these two branches. They show that using \achswlm, we can decompose the distribution over all terms into  highly separable distributions, each one representing the language usage related to the meaning behind the layer of the entity in the hierarchy. 

\begin{figure}[!t]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\linewidth]{02-part-01/chapter-03/figs_and_tables/img_nlm02316.png}
\caption{\label{fig:VSO} \achswlm of S. van Haersma Buma (as the member of parliament - Leader of CDA), CDA (as the party), Opposition (as the status), and the Parliament.}
        \end{subfigure}
        ~~~~~~~~
        \begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\linewidth]{02-part-01/chapter-03/figs_and_tables/img_nlm02335.png}
\caption{\label{fig:VSC} \achswlm of D. Samson (as the member of parliament - Leader of PvdA), PvdA (as the party), Government (as the status), and the Parliament.}
        \end{subfigure}
        \caption{\label{fig:VS} \emph{Vertical Separability}: probability distribution over terms in different layers based on \hswlms in complete paths from the root to the terminal entities in the hierarchy.}
\end{figure}

The two-dimensional separation property of \achswlm in the hierarchy is essentially due to the parsimonization effect in two directions. 
Intuitively, the horizontal separability is mainly the result of the specification stage. For example, when an entity is parsimonized given its direct parent, since the data in its parent is formed by pooling the data from the entity and its siblings, parsimonization makes the representation of the entity separable from its siblings, which provide \emph{horizontal separation} in the resulting language models. On the other hand, vertical separability is mainly due to the generalization stage (and implicit specification). For example, when an entity is parsimonized given its children, since they are specified already, parsimonization gets rid of the specific terms of the lower layer from the entity's representation.


%---------------------------------------
\subsection{Separability for Transferability}
\label{subsec:Separability}
%---------------------------------------
As an extrinsic evaluation of the \hswlms, we investigate the effectiveness of the learned representations in a classification task in the parliamentary dataset with an evolving hierarchical structure. The task is to predict either the party that a member of the parliament belongs to, or the status of the member's party, having all the speeches given by that member in a period,  as well as all the speeches given by the members of all parties in a different period of parliament.

In the parliament, the composition of parties and statuses changes over different periods (Figure~\ref{fig:DutchParl}) and hence the speeches related to different entities can vary dramatically.  Due to this fact, cross period classification is notoriously challenging \citep{Hirst:2014,yu:2008}.  We show that representing entities with \achswlm tackles the problem of having non-stable representations when the composition of parliament evolves during the time, by capturing the essence of language models of parliamentary entities at aggregate levels. 

We use SVM as the base classifier.We trained a standard SVM on raw text as well as a SVM that uses the probabilities of terms in \achswlm as the weights of features. Using the probabilities estimated by \achswlm as weights for features can be considered as a feature selection approach that filters out features that are not essential in accordance to the hierarchical position of entities and make the data representation more robust by taking out non-stable terms.\footnote{We have also tried SVM along with a feature selection methods~\citep{Forman:2003,brank:2002} that uses the Information Gain (IG) to select features as a baselines and reported the results in~\citep{Dehghani:2016:ICTIR}.}
We have employed conventional 5-fold cross validation for training and testing and to maintain comparability, we have used the same split for folding in all the experiments.

\input{02-part-01/chapter-03/figs_and_tables/table_corss_period_party.tex}
\input{02-part-01/chapter-03/figs_and_tables/table_corss_period_status.tex}

Tables~\ref{tbl:swlm-status} and~\ref{tbl:swlm-party} show the performance of employing \acswlm on status and party classification, respectively. 
Tables~\ref{tbl:SVM-status} and~\ref{tbl:SVM-party} indicate the results of SVM classifier on status and party classification respectively. Comparing the results in Tables~\ref{tbl:SVM-status} and ~\ref{tbl:swlm-status}, we see that the accuracy of SVM in within period experiments is sometimes slightly better, but in cross period experiments, the classifier that uses \acswlm of statuses achieves better results.  This is also observed in the results in Table~\ref{tbl:swlm-party} compared to the results in Table~\ref{tbl:SVM-party}. 
%

For party classification, employing \acswlm results in a significant improvement over the baseline.  \citet{Hirst:2014} discuss that since the status of members in parliament, compared to their party, has more effect on the content of their speeches, classifiers tend to pick features related to the status, not the party ideologies. So, SVM performs very well in terms of accuracy in the within-period experiments, but this performance is due to the separability of parties due to their status. Hence, changing the status in cross period experiments, using the learned representation on other periods fails to predict the party, so the accuracies drop. This is exactly the point where our proposed method kicks in. 
Since for each party, the \acswlm is less affected by the status of the party in that period, the representation remains valid even when the status is changed.  In other words, eliminating the effect of the status layer in the party representation in the specification stage ensures that the party language model captures the essential terms related to the party ideology, not its status. Thereby, it is a stable representation that is transferable over time.
%
We conducted a one-tailed t-test on the results. In both party and status classification, in all cases where \acswlm performs better than the SVM, the improvement is statistically significant ({p-value} $<$ 0.005).

To get a better intuition of the procedure of estimating \acswlm, consider the hierarchical relations of Dutch parliaments in the period of \emph{2006--2010}, which is depicted in Figure~\ref{fig:DutchParl}. 
%
Assume that the goal is modeling language usage of ``Christian-Union (CU)'' as an entity in the party layer. In the speeches from the members of this party, words like ``\emph{Chairman}'' or ``\emph{Agree}'' might occur repeatedly. However, they are not a good point of reference for the party's ideological language usage. In the procedure of estimating \acswlm\ of the ``Christian-Union,'' these words are removed from the initial estimated standard language model in the specification stages, since ``\emph{Chairman}'' is a general term in the parliamentary domain and is only able to explain the root entity and ``\emph{Agree}'' is somehow an indicator of language usage of all the ``Government'' parties.
%
On the other side, assume that the goal is to model language usage of ``Government'' as an entity in the status layer. Speeches from ``Christian-Union'' members, which are also counted as ``Government'' members, may contain words like ``\emph{Bible}'' or ``\emph{Charity}.''  It is trivial that involving these party-specific words in the learned representation for the ``Government'' in an individual period hurts the comprehensiveness. In the procedure of estimating \acswlm for the ``Government,'' in the generalization stages, these words are discarded from the representation. This way, the ``Government'' representation does not lose its validity on other periods where the ``Christian-Union'' is not in a Government party.

As another indicator of the effectiveness of \acswlm, it outperforms the SVM bringing all the data together from three different periods in both party and status classification. This is because it gets the chance of having richer training data, which leads to more precise models. While in SVM, changes in the parliamentary composition make speeches diverse, as a result of which it is not able to learn a concrete model. 
\subsection{Invariance of the Representations}
\input{02-part-01/chapter-03/figs_and_tables/plot_cross_period_party_rep_divergence.tex}
As an intrinsic evaluation of the models, we evaluate the invariance of representations learned by our model over different periods---how similar are representations of a particular entity in the hierarchy when trained on data from different periods. 
%

To assess this, we use the diversity of entities' representations in different periods to measure their (in)variance over time.
%
First, all \achswlms from different periods of each party and each status are smoothed using Jelinek-Mercer smoothing \citep{Zhai:2001} considering all parliamentary speeches in the corresponding period as the background collection and with the same value of the smoothing parameter. Then, we use the Jensen-Shannon divergence as the diversity metric to measure dissimilarities between each two {\achswlm}s learned from different periods and then calculate the average of values for each entity. As the baseline, the same calculation is done for the standard language models of entities, i.e., language models estimated using maximum likelihood estimation. 
Figure~\ref{fig:cross_period_party_rep_divergence} shows the diversity of representations in different periods.
%
As can be seen, in all entities in both party and status layers, the diversity of \achswlm of different periods is lower than the diversity of standard language models, which shows that the estimated {\achswlm}s are more invariant over different periods. 


In order to better understand the results in the previous section, we zoomed in on the period of 2010--2012 and 2012--2014 and looked into the confusion matrices of cross period experiments and observed that most of the errors made by SVM are misclassifying members of CDA to PvdA and vice versa. These are the two parties whose statuses have been changed in these periods.  

\input{02-part-01/chapter-03/figs_and_tables/plot_cross_period_rep_divergence.tex}
We investigate representations of these two parties to understand how separation in the feature representation affects the performance of cross period classification. To do so, for each of these two classes, in each period, we extract three probability distributions on terms indicating their importance based on different weighting approaches: 1) Term Frequency (used as feature weights in SVM) and 2) probability of terms in \achswlm (used as feature weights in $SVM_{\achswlm}$). 
Then, as a metric to measure separability of features, we use the Jensen-Shannon divergence to calculate diversity of probability distributions in three cases: 1) Different Parties in the Same Period, 2) Same Party in Different Periods 3) Different Parties in Different Periods. 
To avoid the effect of the number of features on the value of divergence, we take the top-$500$ highest scoring terms of each of the weighting methods as their fixed length representatives. Figure~\ref{fig:cross_period_rep_divergence} shows the average diversity of distributions in each of the three cases for each of the three weighting methods.

As expected, the diversity of features for different parties in a single period is high for both methods. However, when we calculate the diversity of features for a single party in different periods, feature representations are different in  $TF$, which causes false negative errors in the classification of these two parties. 
When using representations using $TF$, we get similar representations for different parties in different periods, probably due to the fact that they share status. This can lead to false positive errors in the classification. 

Considering these observations together reveals that SVM learns representations on the basis of features that are indicators of issues related to the status of parties, since they are the most discriminating terms considering one period and in within period experiments, the performance of SVM is due to the separability of parties based on their statuses. Hence, after changing the status in the cross period experiments, the trained model of the previous period generated by SVM fails to predict the accurate party.  In the same way, the status classifier is affected by different parties forming a government in different periods, leading to lower accuracies.   

This is exactly the point where the strengths of \achswlm kick in. In fact, two-dimensional separability in the feature representation enables $SVM_{\achswlm}$ to tackle the problem of having non-stable features in the representation when the status of a party changes over time. In other words, eliminating the effect of the status layer in the party representation, which is the result of the horizontal separation, ensures that the party representation captures the terms related to the party ideology, not its status. Thereby, $SVM_{\achswlm}$ is not only effective in within period classification, but also the models learned on the data from one period remain valid when the statuses of parties change in other periods. 
