\section{\HSWLMs}
\label{sec:hswlm}
In this section, we address the second research question of this chapter:
\resq{c3.2}

We introduce \HSWLMs (\achswlm), which is an extension of \SWLM that is explained in Chapter~\ref{chap:2} tailored to textual hierarchical entities. \achswlm is, in fact, a particular arrangement of multiple passes of the procedures of making representations of hierarchical entities vertically and horizontally separable, as they are explained in Section~\ref{subsec:vhs}.
%
We use the idea of parsimonious language models \cite{Hiemstra:2004} to parsimonize the representation of an entity by down-weighting terms that do not reflect the significant features of that entity, with regards to its position in the hierarchy.   
In the parsimonious language model, given a raw probabilistic estimation, the goal is to reestimate the distribution so that non-essential parameters of the raw estimation are down-weighted if they are well explained in a given background distribution. 
The proposed approach for estimating \hswlm iteratively reestimates the standard language models of entities to minimize their overlap by discarding non-essential terms from them. 

In the original parsimonious language model~\citep{Hiemstra:2004}, the background model is explained by the estimation of the \emph{collection model}, i.e., the model representing all the entities, similar to Equation~\ref{eq:cmodel}.  
However, with respect to the hierarchical structure, and our goal in \achswlm for making the representations of entities separable from each other, we need to use the parsimonization technique in two different directions: 1) given ancestors of an entity, and 2) given its descendants. Hence, besides parsimonizing given a single parent entity in the upper layers, as the background model, we need to be able to do parsimonization given multiple descendants in the lower layers. 
\input{02-part-01/chapter-03/figs_and_tables/alg_hswlm_model_parsimonization.tex}
Algorithm~\ref{alg:model_parsimonization} presents pseudo-code of the Expectation-Maximization algorithm that is employed in the modified model parsimonization procedure. 
In the equation in line 3 of the pseudo-code in Algorithm~\ref{alg:model_parsimonization}, $B$ is the set of background entities ---either one or multiple, and $\theta_{b_i}$ denotes the model of each background entity, $b_i$, which is estimated using MLE. In case of having a single ancestor node as the background model,  this equation will be equal to Equation~\ref{eq:cmodel} and in case of having multiple descendants as background models, it results in Equation~\ref{eq:smodel}. 
In this procedure, in general, in the E-step, the probabilities of terms are adjusted repeatedly, and in the M-step, adjusted probabilities of terms are normalized to form a distribution. 
Another change in the modified version of model parsimonization, which practically makes no difference in the final estimation, is that in the E-step, instead of using $tf(t,e)$, we employ $p(t|\theta_e)$, where $\theta_e$ is the language model that represents entity $e$ and initially it is estimated using MLE. This is because in the multi-layer hierarchies, there is more than one parsimonization pass for a particular entity and after the first round, we need to use the probability of terms estimated from the previous pass, not the raw information of their frequency.

\input{02-part-01/chapter-03/figs_and_tables/alg_hswlm.tex}
Model parsimonization is an almost hyper-parameter free process. The only hyper-parameter is the standard smoothing parameter $\lambda$, which controls the level of parsimonization, so that the lower values of $\lambda$ result in more parsimonious models.
The iteration is repeated a fixed number of times or until the estimates do not change significantly anymore. 

The pseudo-code of the overall procedure of estimating \achswlm is presented in Algorithm~\ref{alg:hswlm}. 
Before the first round of the procedure, a standard estimation like maximum likelihood estimation is used to construct the initial representation for each entity in the hierarchy. 
Then, representations will be updated in an iterative process until all the estimated representations of entities become stable. 
In each iteration, there are two main stages: a \emph{Specification stage} and a \emph{Generalization stage}. 
In these stages, language models of entities in the hierarchy are iteratively made ``specific,'' by taking out terms explained at higher levels, and ``general,'' by eliminating specific terms of lower layers, which results in representations that are both \emph{horizontally} and \emph{vertically} separable as it is described in Section~\ref{subsec:vhs}.

In the specification stage, the goal is to eliminate the general terms of the language model of each entity so that the resulting language model demonstrates the entity's specific properties.  
To do so, the parsimonization method is used to parsimonize the language model of an entity given its ancestors, from the root of the hierarchy to its direct parent, as the background estimations. 
%
The order in the hierarchy is of crucial importance here. 
When a language model of an ancestor is considered as the background language model, it should demonstrate the ``specific'' properties of that ancestor. Due to this fact, it is important that before considering the language model of an entity as the background estimation, it has already passed the specification stage, and we have to move top-down.
Pseudo-code of the recursive procedure of specification of entities' representations in the hierarchy is depicted in Algorithm~\ref{alg:specification_stage}.

\input{02-part-01/chapter-03/figs_and_tables/alg_hswlm_specification_step.tex}
\input{02-part-01/chapter-03/figs_and_tables/alg_hswlm_generalization_step.tex}

In the generalization stage, the goal is to refine language models by removing terms that do not address the concepts in the level of abstraction of the entity's layer.
To do so, again parsimonization is exploited but given descendants, which leads to the elimination of specific terms. 
Here also, before considering the representation of an entity as the background model, it should be already passed the generalization stage, so generalization moves bottom up.
Algorithm~\ref{alg:generalization_stage} presents the pseudo-code for the recursive procedure of generalization of entities' language representations in the hierarchy. 
In the generalization step, the background models of descendants are supposed to be specific enough to show their extremely specific properties. Hence, generalization stages must be applied to the representations that are output of specification stages: specification should precede generalization, as shown in Algorithm~\ref{alg:hswlm} before.
