\chapter{\titleof{c3}}
\label{chap:3}
%
\begin{quote}
Hierarchies are powerful structures to model different levels of associations in the data. They can evolve over time in term of the relations between entities in the hierarchy and learning representations that are agnostic to these changes are not trivial. We can, however, take the horizontal and vertical dependencies in a hierarchy into account and learn highly separable representations for entities, that are less variant to structural changes and more transferable over time. 
\end{quote}
%
\section{Introduction}
Hierarchy is an effective and common way of representing information, and many domains are naturally organized in a hierarchy. Organizing data in a hierarchical structure is valuable since it determines relationships in the data at different levels of resolution and picks out different categories relevant to each of the various layers of memberships.  
Besides that, using hierarchies is an effective way of representing the information as it eases the task of \emph{comparison}, which is a critical factor in analogical reasoning. For example, the problem of deciding whether two entities are analogous can be formalized as the problem of checking the level of abstraction at which these entities are instances of the same node in a hierarchy.

Taking advantage of the structure in a hierarchy requires modeling and representing entities, taking their relationship in the hierarchy into consideration. 
There are two types of dependencies in the hierarchies: i) \emph{Horizontal dependency}, which refers to the relations of entities in the same layer.  A simple example would be the dependency between siblings which have some commonalities in terms of being descendants of the same entity. ii) \emph{Vertical dependency}, which addresses the relations between ancestors and descendants in the hierarchy. For example the relation between root and other entities. 

Due to the existence of two\:--\:dimensional dependencies between entities in the hierarchy, modeling them regardless of their relationships might result in overlapping models that are not capable of making different entities distinguishable.  Learning representations with minimal overlap is one of the requirements for collision resistance systems and when the representations are not well\:-\:separated, classification and retrieval systems are less likely to work well~\citep{Lewis:1992}. 
Thus, \emph{two\:-\:dimensional separability}, i.e.\ \emph{horizontal and vertical separability}, is one of the key requirements of hierarchical classification.

In this chapter, we focus on one of our research questions:
\resq{c3}

We introduce \hswlms, which extends the idea of \swlms to the hierarchical structure. We assume that entities, like people, organizations, concepts, and ideologies are organized in a hierarchy, and for each entity, there are textual data associated with the entity with respect to the subhierarchy under the entity, where the data is generated only at the leaves of the hierarchy. Each entity, which is a node in the hierarchy, is represented by a specific probabilistic language model.

Given the hierarchical structure and the data associated with entities in the hierarchy, \achswlm iteratively sparsifies the representation of the entities by discarding features that are well explained by their ancestors, i.e., general features, as well as features that reflect the characteristics of individual descendants, i.e., specific features. This leads to representations for entities that are both vertically and horizontally separable, in terms of their position in the hierarchy, as they capture only the significant features of entities.

\input{02-part-01/chapter-03/figs_and_tables/fig_eg_parl_hierarchy.tex}
As a concrete example, consider a simple hierarchy of a multi\:-\:party parliament as shown in Figure~\ref{fig:ParHierarchy}, which determines different categories relevant to the different layers of membership in the parliament.  We can associate an individual member of parliament by her speeches, a political party by their member's speeches, the opposition by the speeches of members of opposition parties, etc. 
In order to represent a party in this hierarchy, a proper model would show common characteristics of its members\:---\:not members of other parties (\emph{horizontal} separation), and capture the party's generic characteristics\:---\:not unique aspects of the current members captured in the individual member's layer or aspects of whether the party is in government or opposition captured in the status layer (\emph{vertical} separation).

\subsection{The Importance of Representational Separability}
The concept of \emph{separability} is of crucial importance especially when the task is not just ranking a set of items, but making a boolean decision about the labels of each item in the set.
Regarding this concern, \citet{Lewis:1995} has presented the Probability Threshold Principle (PTP), as a stronger version of the Probability Ranking Principle~\citep{Robertson:1977}, for binary classification, which discusses optimizing a threshold for separating items regarding their probability of class membership. 
PTP is a principle based on the separability in the score space. However, here we discuss separability in the data representation and later in this chapter we define a \emph{\ssp} as the counterpart of PTP in the feature space.
\input{02-part-01/chapter-03/figs_and_tables/fig_separable_rep.tex}
Separation in the data or feature space is a favorable property that not only helps to improve for ranking or classification algorithms but also brings out characteristic features for human inspection.
Figures~\ref{fig:sep-slm} and ~\ref{fig:sep-swlm} illustrate two different ways of modeling two entities in the status layer of the parliamentary hierarchy, i.e., government and opposition. 
Each model is a probability distribution over terms (language model) based on the speeches given by all the members in the corresponding status. In each figure, we sort the terms based on their weights in one of the models and plot the other in the same order. 
%
As can be seen, although distributions over terms in Figure~\ref{fig:sep-slm} for two classes are different, they do not suggest highly separable representations for classes. However, estimated language models in Figure~\ref{fig:sep-swlm} provide highly separable distributions over terms for two classes, identifying the characteristic terms that uniquely represent each class, and can be directly interpreted.  
 
Besides effectiveness and intractability, two\:-\:dimensional separability in the representation of hierarchical entities leads to robust models. This is due to the fact that these representations only capture solid features that remain valid even as the structure of hierarchy changes.
As an example of the importance of robust representations in evolving hierarchies, assume we would learn a representation for the ``US president'' over the current data. This is obvious that we need to distinguish the role in office from the person who is the current president; otherwise the learn representation would not be valid after the next election.  If we can separate the model of the function from the model of the person fulfilling it, for example by abstracting over several presidents, that would in principle be robust over time.

\subsection{Detailed Research Questions}
We break down our main research question in this chapter into three concrete research questions:
\begin{resqbox}
\begin{enumerate}
\item[\textbf{\resqname{c3.1}}] \emph{\resqcontent{c3.1}}
\item[\textbf{\resqname{c3.2}}] \emph{\resqcontent{c3.2}}
\item[\textbf{\resqname{c3.3}}] \emph{\resqcontent{c3.3}}
\end{enumerate}
\end{resqbox}
In the following sections, we will address these research questions.

\input{02-part-01/chapter-03/theoritical_discussion.tex}
\input{02-part-01/chapter-03/model.tex}
\input{02-part-01/chapter-03/applications.tex}
\input{02-part-01/chapter-03/related_work.tex}


\section{Conclusion}
Building conceptually accurate models of data with a structure consisting of multiple layers, or a hierarchy, prompts us to analyze them at different abstraction levels.  However, this requires the ability to estimate separable models for hierarchical entities which capture their essential features taking into account their relative position in the hierarchy. 

In this chapter, we focused on addressing \textbf{\resqname{c3}}. We demonstrated that based on the ranking and classification principles, the \emph{separation property} in the data representation is a desirable foundational property which leads to separability of scores and consequently improves the accuracy of classifiers' decisions, which addressed \textbf{\resqname{c3.1}}.  We stated this as the ``\ssp'' for optimizing expected effectiveness of classifiers.

We showed that in order to have horizontally and vertically separable models, they should capture all, and only, the essential terms of the entities taking their position in the hierarchy into account. Based on this, to address addressed \textbf{\resqname{c3.2}}, we introduced \hswlms for estimating separable models for hierarchical entities. We studied \achswlm and demonstrated that it offers separable distributions over terms for different entities both in case of being in the same layer or in different layers.  To study \textbf{\resqname{c3.3}}, we evaluated the performance of classification over time using separable representations of data and showed that separability makes the model more robust and transferable over time by filtering out non-essential non-stable terms.

\bigskip
%conclusion of part 1 and connection to part 2
In part~\ref{part1} of the thesis, we focused on addressing \textbf{\resqname{p1}} by studding how incorporating some prior knowledge can help improving the robustness of the outcome of the learning process in noisy and variable environment. We showed that taking the structure of the data, which is in fact a form of inductive bias, can help learning effective representations. In Part~\ref{part2}, we investigate how we can change the learning process to make it more robust against the noise in the weakly annotated labels.