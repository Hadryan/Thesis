\section{\SWLMs}
\label{sec:swlm}
In this section, we address the first research question of this chapter: 
\resq{c2.1}

We introduce \SWLMs and describe how to estimate them. \acswlm assumes that terms in the associated documents are drawn from three models: 1.~A \emph{general model}, representing common observations, 2.~A \emph{specific model}, representing partial observations, and 3.~A \emph{significant words model}. 

The significant words model is the latent model representing the essential features of the entity. The general and specific models, however, are not necessarily topic-centric models. In a way, they are supposed to represent two distributions of terms that are not considered as significant information. 

Each model is represented using a terms distribution, i.e., a unigram language model, $\theta_{sw}$, $\theta_g$, and $\theta_s$. 
We assume that each term in a document in the set is generated by sampling from a mixture of these three models independently. Thus, the probability of appearance of the term $t$ in the document $d$ is as follows:
\begin{equation}
p(t|d) =  \lambda_{d,sw} p(t|\theta_{sw}) + \lambda_{d,g} p(t|\theta_g) + \lambda_{d,s} p(t|\theta_s),
\end{equation}
where $\lambda_{d,x}$ stands for $p(\theta_x|d)$ which is the probability of choosing the model $\theta_x$ given the document $d$. 

We estimate the general and specific models based on patterns of occurrences of terms in different documents in the set and fix them in the estimation process as infinitely strong priors. 
We consider the collection model, i.e., the set of all documents, $\theta_C$ as an estimation for $\theta_g$:
\begin{equation}
p(t|\theta_g) = 
p(t|\theta_C) = \frac{c(t,C)}{\sum_{t' \in V} c(t',C)},
\end{equation} 
where $c(t,C)$ is the frequency of term $t$ in the collection. This way, terms that are well explained by the collection model get high probability and are considered as general terms.

Furthermore, we establish a definition for ``specificity'' with regards to our main goal, which is estimating a representation for a set of documents, as being supported by part of the documents in the set but not all. We estimate $\theta_s$ to represent the probability of a term being partially observed as follows, and normalize all the probabilities to form a distribution:
\begin{equation}
p(t|\theta_s) = 
\sum_{d_i\in \mathcal{D}} 
\bigg(
p(t|\theta_{d_i}) \prod_{\substack{d_j\in \mathcal{D} \\ j \neq i}} (1-p(t|\theta_{d_j}))
\bigg),
\label{theta_s}
\end{equation}
where $P(t|\theta_{d_i}) = \nicefrac{c(t,d_i)}{\sum_{t' \in d_i} c(t',d_i)}$. 
Intuitively, Equation~\ref{theta_s} defines the probability of term $t$ to be a \emph{specific} term, based on how much this term is important in one of the documents but not others, marginalizing over all the documents in the set $D$. This way, terms that are well explained in only one document but not others get higher probabilities and are considered as insignificant specific terms.

Having the above assumptions, the goal is to fit a log-likelihood model of generating all terms in the documents in the set to discover the term distribution of the significant words model, $\theta_{sw}$. 
Let $\mathcal{D} = \{d_1, \ldots, d_{\mathcal{D}}\}$ be the set of documents associated withe the entity we want to learn a representation for. The log-likelihood function for the entire set of documents is:
\begin{equation}
\log p(\mathcal{D}|\Upsilon) = \sum_{d \in \mathcal{D}}\sum_{t \in V} c(t,d) \log \left( \sum_{x\in\{sw,g,s\}}\lambda_{d,x} p(t|\theta_x)\right),
\end{equation}
where $c(t,d)$ is the frequency of the term $t$ in the document $d$, and $\Upsilon$ determines the set of all parameters that should be estimated, $\Upsilon =\{\lambda_{d,sw}, \lambda_{d,g}, \lambda_{d,s} \}_{d \in \mathcal{D}} \cup \{\theta_{sw}\}$. 

To fit our model, we estimate the parameters using the maximum likelihood (ML) estimator. Therefore, assuming that documents are represented by a multinomial distribution over the terms, we solve the following problem:
\begin{equation}
\Upsilon^* = \argmax_\Upsilon p(\mathcal{D}|\Upsilon)
\end{equation}
Assuming that $X_{d,t} = \{{sw},g,s\}$ is a hidden variable indicating which model has been used to generate the term $t$ in the document $d$, we can compute the parameters using the Expectation-Maximization (EM) algorithm. 
The stages of the EM algorithm are as follows:
\begin{description}
\item[E-Step]
\begin{equation}
p(X_{d,t} = x) = \frac{p(\theta_x|d)p(t|\theta_x)}{\sum_{x' \in \{sw,g,s\}}p(\theta_{x'}|d)p(t|\theta_{x'})}
\label{EM_e}
\end{equation}
\item[M-Step]
\begin{equation}
p(t|\theta_{sw}) = 
\frac{\sum_{d \in \mathcal{D}}c(t,d) p(X_{d,t} = r)}{\sum_{t' \in V}\sum_{d \in \mathcal{D}}c(t',d) p(X_{d,t'} = r)}
\label{EM_m1}
\end{equation}
\begin{equation}
\lambda_{d,x}  = p(\theta_x|d) = 
\frac{\sum_{t \in V}c(t,d) p(X_{d,t} = x)}{\sum_{x' \in \{sw,g,s\}}\sum_{t \in V}c(t,d) p(X_{d,t} = x')}
\label{EM_m2}
\end{equation}
\end{description}

\input{02-part-01/chapter-02/figs_and_tables/fig_plate_diagram.tex}
Figure~\ref{fig:graphical-model} represents the plate notation of \acswlm. As it is shown, for each document the contribution of each of the three models, i.e., $\lambda_{d,x}$ for $x \in \{sw,g,s\}$, are estimated. It can be seen that the general model, $\theta_g$, and the specific model, $\theta_s$ are considered as external observations, which are involved in the estimation process as infinitely strong priors. 

In the next section, we employ \acswlm in different applications in order to assess its effectiveness in modeling an entity given a set of documents that are associated with the entity. We evaluate \acswlm on relevance feedback in the retrieval task, where we need to model the set of relevant documents or top-ranked documents in the initial retrieved results and use this model to expand the user's query to improve the retrieval performance by shrinking the vocabulary gap between query and relevant documents. Furthermore, we use \acswlm as a group profiling approach in the task of contextual suggestion to model preferences of a group of people and augment user profiles with the profiles of implicit or explicit groups they belong to.