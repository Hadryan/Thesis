% !TEX root = ../thesis-main.tex
\part{\titleof{p3}}
\label{part3}
Generalization is at the heart of many aspects of human cognition. It underlies our ability to learn language, form and use categories, and learn about causal relationships, while in many cases we are presented only with limited observation. The great ability of generalization in human cognition sets a standard to which artificial intelligence and machine learning research aspires.

This immediately raises questions like ``How can human learn so much about the world from such limited evidence?'' and ``What makes human so good at generalization?''.
The importance of generalization in cognitive science partly derives
from its close relationship to inductive inference.  We can define generalization as forming predictions about future events based on examples from the past, which emphasizes its relationship to the classic problem of induction~\citep{hume2003treatise}. 
Having this connection between generalization and induction in mind, we can cast the question of ``how to produce good generalizations?'' to the question of ``what makes a good inductive learner?''.

To answer this question, from human cognition perspective, as human develop, they become capable of exploring ``more sophisticated hypotheses'' about the structure of their environment~\citep{inhelder1958growth}, which allows them to better infer their reality status and become better inductive learners. However, human sometimes has to make decisions without information from their senses or testimony from others, no matter how rich is their hypotheses space.
In these cases, their internal dispositions, i.e. ``inductive biases'', provides a source of knowledge that influences their decisions in the absence of experience, explicit sensory or testimonial proof~\cite{sodian1987children,griffiths2010probabilistic}.

In the context of learning algorithms, given the data $d$, the learner aims at identifying the hypothesis $h$ from a set of hypotheses $H$ that results in the highest generalization accuracy.  Having similar trend to the human cognition, algorithms that are able to explore ``richer hypothesis spaces'' have the potential of being better inductive learners\footnote{The idea of expansion of the hypothesis space by adding more layers and non-linearity to neural networks to make it possible to overcome the constraint of linear separability.}. However, in many cases, the considered hypotheses are not directly defined by the observed data and to choose among the many hypotheses that are equally granted by the data, the learner has to inject some preferences for those hypotheses that are called ``inductive biases''~\footnote{Biases can be both in inductive and in deductive systems. Systems that learn concepts from labeled training instances employ \emph{inductive bias}}. 

\emph{Inductive biases} are, in fact, factors that lead a learner to favor one hypothesis over another that are independent of the observed data. When two hypotheses fit the data equally well, inductive biases are the only basis for deciding between them and making it possible to generalize beyond the observed data~\citep{Mitchell80theneed}.

This brings us to the discussion of the \emph{bias-variance trade-off}.
We can decompose the expected generalization error into two parts: the
bias of a learning algorithm, and its variance~\citep{geman1992neural}. The transition from one source of error to another is known as the \emph{bias-variance trade-off}, and much of the research in designing learning algorithms is about trying to find the sweet spot between bias and variance for a given problem. 
%The generalization error can be attributed to a combination of intrinsic error due to the noise in output space, and systematic error resulting from the difference between the true function and the function selected by the learning algorithm.

The bias-variance trade-off suggests that the generalization of a learning algorithm depends on the problems at hand and different factors involved in learning, like the amount of available data. 
If the learner is provided with only small amounts of data, then the variance is the real concern and richer hypothesis spaces may hurt the generalization because they increase variance.  To increase the chance of generalization in these cases is to inject inductive biases with respect to the problem at hand.
However, if the learner is provided with large amounts of data and needs to be able to solve a variety of problems, then variance is less of an issue and the bias can be the dominant source of error, thus the learner needs richer hypothesis spaces to be flexible enough to accommodate the different solutions, as this reduces bias.

For neural networks, the inductive biases inherent in their architecture is perhaps the most important factor determining how quickly they train and how well they generalize beyond the data they observed during training. 
A well-known example is the translation invariant assumption in the convolutional neural networks~\citep{lecun1989backpropagation} for vision tasks based on a certain symmetry in the data, which considers that a given feature, of any complexity, can appear anywhere in the image.   Other examples include neural networks that encode rotational invariance~\citep{cohen2016steerable} or permutation invariance \citep{zaheer2017deep, lee2018set}.
Having inductive biases gets even more crucial when we need data \emph{efficient models} that are able to \emph{generalize beyond observed training data} and can \emph{learn complex tasks} like tasks that need reasoning or inferring an underlying structure from the data.

Among the sequence processing neural networks, recurrent neural networks have long been the de facto choice for sequence modeling tasks. The most important facet of RNNs is the \emph{recurrence} which lets the model updates its internal state in a loop given the input at each time step. The recurrence is simply using information from the previous state which in turn uses the previous state so on and so forth.  In other words, RNNs implement the ``re-occurrence'' of referring back to all previous internal states. This adds a ``recurrent inductive bias'' to the model that may be crucial for several algorithmic and language understanding tasks~\cite{tran2016recurrent,Dehghani:ICLR:2019}.

However, the recurrence in time dictates the inherently sequential computation which makes RNNs slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation~\citep{vaswani2017attention, NalBytenet2017}, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer~\citep{vaswani2017attention} fail to generalize in many simple tasks e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time, while recurrent models handle these tasks with ease because of the inductive recurrent bias.

In Part~\ref{part3} of this book, we address the following research question:
\resq{p3}

Here, as a model with no recurrence in time, we focus on Transformer~\citep{vaswani2017attention} as a successful sequence model on many language understanding and generation tasks. In Chapter~\ref{chap:6}, we study how lack of recurrent inductive bias in Transformer may lead to the failure of the model on complex reasoning tasks with limited data, algorithmic tasks where length generalization over training samples is needed, and structured language understanding tasks, and address the following research question:
\resq{c6}

We introduced Universal Transformer~\citep{Dehghani:ICLR:2019}, a self-attentive concurrent-recurrent sequence model, which is an extension of Transformer model~\citep{vaswani2017attention}. The Universal Transformer introduces recurrence in depth by repeatedly refines a series of vector representations for each position of the sequence in parallel, by combining information from different positions using self-attention and applying a recurrent transition function across all time steps. 
In the simplest form, Universal Transformer with a fixed number of iterations is almost equivalent to a multi-layer Transformer with tied parameters across all its layers. By sharing weight, we can save massively on the number of parameters that we are training and fewer parameters means learn faster with fewer data points. We show that the elegant idea of introducing recurrent in depth enables Transformer to extrapolate from training data much better on a range of algorithmic and language underestimating tasks~\cite{Dehghani:ICLR:2019, Dehghani:2019:WSDM}.

Besides the recurrence in depth, we propose a simple inductive bias for UTs that lets the model generalize well to input lengths that are not observed during training.  We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.  It is noteworthy that in contrast to the standard Transformer, under certain assumptions UTs can be shown to be \emph{Turing-complete}. We will discuss this further in Chapter~\ref{chap:6}.

% How can inductive biases be so strong yet so flexible? Nonparametric models, growing in complexity as the data require. 
\input{04-part-03/chapter-06/main.tex}