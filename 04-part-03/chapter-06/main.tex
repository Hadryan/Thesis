\chapter{\titleof{c6}}
\blankfootnote{This chapter is based on \citep{Dehghani:ICLR:2019,Dehghani:2019:WSDM}.}

\label{chap:6}
%
\begin{quote}
Inductive biases are great ways for encoding modeling assumptions and improving data efficiency. For many sequence modeling tasks, the recurrent inductive bias plays a crucial role for generalizing beyond the observed data, while self-attentive feed-forward sequence models forego the recurrence toward parallelizability. We can, however, introduce a form of recurrent inductive bias to these models to improve their generalization while keeping the parallelization in the computations. 
\end{quote}
%

\section{Introduction}
Human child achieves the needed complex linguistic knowledge within a short time, with very limited observation. This cannot be explained easily and relying on the \emph{poverty of stimulus}~\citep{chomsky1980rules} argument, a powerful task-specific bias is required for learning to understand and generate language. In the context of learning algorithms, it has been discussed that without a certain complexity to the learning bias, the training data is insufficient to permit learning a model that generalizes properly to the full range of unseen instances for a specif task~\cite{Mitchell80theneed}. In other words, \emph{data-driven} learning, which merely relies on previous experience has to  with \emph{innately primed} learning, which is based on having the some knowledge encoded into the model in the form of fixed biases, either strong or weak.

Different neural network based models have been proposed for sequence modeling and and employed for language understanding and generation tasks. Among them, convolutional and fully-intentional feed-forward architectures like the Transformer have recently emerged as viable alternatives to recurrent neural networks (RNNs) for a range of sequence modeling tasks, notably machine translation~\citep{JonasFaceNet2017,transformer}. These parallel-in-time architectures address a significant shortcoming of RNNs, namely their inherently sequential computation which prevents parallelization across elements of the input sequence, whilst still addressing the vanishing gradients problem as the sequence length gets longer~\citep{vanishing-exploding-gradient}.
The Transformer model in particular relies entirely on a self-attention mechanism \citep{decomposableAttnModel,lin2017structured} to compute a series of context-informed vector-space representations of the symbols in its input and output, which are then used to predict distributions over subsequent symbols as the model predicts the output sequence symbol-by-symbol. Not only is this mechanism straightforward to parallelize, but as each symbol's representation is also directly informed by all other symbols' representations, this results in an effectively global receptive field across the whole sequence. This stands in contrast to e.g.\ convolutional architectures which typically only have a limited receptive field.

Notably, however, the Transformer with its fixed stack of distinct layers foregoes RNNs' inductive bias towards learning iterative or recursive transformations. Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine~\citep{ntm14}, the Neural GPU~\citep{neural_gpu} or Stack RNNs~\citep{stack_rnn}, the Transformer does not generalize well to input lengths not encountered during training. 

In this chapter, we address the following research question:
\resq{c6}

\begin{figure}
 \centering
 \includegraphics[width=\textwidth]{04-part-03/chapter-06/figs_and_tables/fig_universal-transformer-as-rnn.pdf}
 \caption{The Universal Transformer repeatedly refines a series of vector representations for each position of the sequence in parallel, by combining information from different positions using self-attention (see Eqn~\ref{MultiheadSelfAttention}) and applying a recurrent transition function (see Eqn~\ref{RecurrentTransition}) across all time steps $1 \leq t \leq T$. We show this process over two recurrent time-steps. Arrows denote dependencies between operations. Initially, $h^0$ is initialized with the embedding for each symbol in the sequence. $h^t_i$ represents the representation for input symbol $1 \leq i \leq m$ at recurrent time-step $t$. With dynamic halting, $T$ is dynamically determined for each position (Section~\ref{sec:dynamic-halting}).}
 \label{fig:rec-state}
\end{figure}


We introduce the \emph{Universal Transformer (UT)}, a parallel-in-time recurrent self-attentive sequence model which can be cast as a generalization of the Transformer model, yielding increased theoretical capabilities and improved results on a wide range of challenging sequence-to-sequence tasks. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs, which seems to be better suited to a range of algorithmic and natural language understanding sequence-to-sequence problems. As the name implies, and in contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete (or ``computationally universal'', as shown in Section~\ref{sec:related}).

In each recurrent step, the Universal Transformer iteratively refines its representations for all symbols in the sequence in parallel using a self-attention mechanism~\citep{decomposableAttnModel,lin2017structured}, followed by a transformation (shared across all positions and time-steps) consisting of a depth-wise separable convolution \citep{xception2016,slicenet} or a position-wise fully-connected layer (see Fig~\ref{fig:rec-state}). We also add a dynamic per-position halting mechanism \citep{graves2016adaptive}, allowing the model to choose the required number of refinement steps \emph{for each symbol} dynamically, and show for the first time that such a conditional computation mechanism can in fact improve accuracy on several smaller, structured algorithmic and linguistic inference tasks (although it marginally degraded results on MT). 

Our strong experimental results show that UTs outperform Transformers and LSTMs across a wide range of tasks. The added recurrence yields improved results in machine translation where UTs outperform the standard Transformer. In experiments on several algorithmic tasks and the bAbI language understanding task, UTs also consistently and significantly improve over LSTMs and the standard Transformer. Furthermore, on the challenging LAMBADA text understanding data set UTs with dynamic halting achieve a new state of the art.

\subsection{Detailed Research Questions}
We break down our main research question in this chapter into two concrete research questions:
\begin{resqbox}
\begin{enumerate}
\item[\textbf{\resqname{c6.1}}] \emph{\resqcontent{c6.1}}
\item[\textbf{\resqname{c6.2}}] \emph{\resqcontent{c6.2}}
\end{enumerate}
\end{resqbox}

In the following sections, we will address these research questions.

\input{04-part-03/chapter-06/universal_transformer.tex}
\input{04-part-03/chapter-06/applications.tex}

\section{Conclusion}
In this chapter we focused on addressing \textbf{\resqname{c6}}.
We introduced the Universal Transformer to address \textbf{\resqname{c6.1}}, a generalization of the Transformer model that extends its theoretical capabilities, by introducing the recurrent inductive bias in depth. 
We have employed the Universal Transformer in on a wide range of challenging sequence modeling tasks, such as language understanding but also a variety of algorithmic tasks, to address \textbf{\resqname{c6.2}} and showed that it produces state-of-the-art results by addressing a key shortcoming of the standard Transformer. The Universal Transformer combines the following key properties into one model:

\textbf{Weight sharing}: Following intuitions behind weight sharing found in CNNs and RNNs, we extend the Transformer with a simple form of weight sharing that strikes an effective balance between inductive bias and model expressivity, which we show extensively on both small and large-scale experiments.

\textbf{Conditional computation}: In our goal to build a computationally universal machine, we equipped the Universal Transformer with the ability to halt or continue computation through a recently introduced mechanism, which shows stronger results compared to the fixed-depth Universal Transformer.

By adding computational capacity and recurrence in processing depth, we hope that further improvements beyond the basic Universal Transformer presented here will help us build learning algorithms that are both more powerful, data efficient, and generalize beyond the current state-of-the-art.

In Part~\ref{part3} of the thesis, we explored the idea of injecting inductive biases into learning algorithms to improve their data-efficiency and generalization. This in fact is defining an innate prior knowledge for the learning algorithms that can eventually help overcoming the challenging problem of poverty of stimulus.

