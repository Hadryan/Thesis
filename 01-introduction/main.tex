% !TEX root = ../thesis_main.tex
\chapter{Introduction}


This is considered as a part of the bigger effort to democratize AI by making it dramatically easier to extend AI-powered systems to domains with limmited data.
http://jmlr.org/papers/volume17/gulchere16a/gulchere16a.pdf.

https://medium.com/center-for-data-science/neural-networks-and-toddlers-how-learning-biases-can-improve-word-learning-56e477dc1ee3


\url{https://www.princeton.edu/~nivlab/papers/GershmanNiv_inpress.pdf}

\section{Problem Description and Research Questions}

Strong prior knowledge (inductive bias). How does background knowledge guide learning from sparsely observed data?  What form does the knowledge take, across different domains and tasks?
How is that knowledge itself learned? 
\resq{main}

Learning from weakly annotated examples is an increasingly popular approaches for addressing the labeled data scarcity in many tasks and applications. 
Weak supervision is usually refers to higher-level approaches to labele training data that are cheaper and/or more efficient, such as:
\begin{itemize}
    \item distant or heuristic supervision,
    \item incidental signals that  exist in the data and the environment, independently of the tasks and they are co-related to the target tasks.
    \item supervising by specifying constraints that should hold over the output space,
    \item using noisy labels, 
    \item pool limited supervision signal by using multi-task learning,
    \item data augmentation strategies to express class invariances,
    \item introduction of other forms of structured prior knowledge.
\end{itemize}
An overarching goal of such approaches is to use domain knowledge and data resources provided by subject matter experts, but to solicit it in higher-level, lower-fidelity, or more opportunistic ways.

can be exploited, along with appropriate algorithmic support, to provide sufficient supervision and facilitate learning

This general research problem leads to
the following main research question of this PhD thesis (RQ-main) 

\section{Thesis Overview}
This thesis consists of the introductory matter, followed by five main
chapters that are divited into three parts, each based on one of the three research questions described above.
Finally, it closes off with the Conclusions, Appendices and Bibliography. 


This thesis is about improving the learning procedure when the data and/or the supervision signal is noisy or limited, mainly in the context of language understanding tasks. The thises has 5 chapters (besides Introduction and conclusion), divided into 3 main parts. The following is a general outline of the thesis, a short summary of each part and the list of papers included in each chapter: 
PART I: Structure of the Data as Prior Knowledge
In this part, we explore how taking the general structured of the data into account can help to estimate representations that capture only the significant features when the data is noisy and highly variant. We break this into two chapters:
Chapter 2: Learning neither General, nor Specific, but Significant Representations 
Mostafa Dehghani, H. Azarbonyad, J. Kamps, D. Hiemstra, and M. Marx. "Luhn Revisited: Significant Words Language Models", CIKM'16.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx "Generalized Group Profiling for Content Customization", CHIIR'16.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx, "Significant Words Language Models for Contextual Suggestion", TREC  2016.
Mostafa Dehghani, "Significant Words Representations of Entities", SIGIR-Doctoral Consortium'16 -- (SIGIR Doctoral Consortium Award).
Chapter 3:  Representational Separability for Hierarchically Structured Data
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx, "On Horizontal and Vertical Separation in Hierarchical Text Classification", ICTIR'16 (Best Paper Award).
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx, "Two-Way Parsimonious Classification Models for Evolving Hierarchies", CLEF'16 (Best Paper Honorable Mention).

PART II: Learning with Weak Supervision.
In this part, we study how human can supervise machine learning systems, by labeling training data programmatically instead of labeling by hand and discuss how to design neural networks that learn to go beyond the imperfection in the weakly annotated data, for instance by meta learning the quality of the labels and modulating the learning process based on the label quality. We break this into two chapters:
Chapter 4: Learning from Pseudo-Labels
Mostafa Dehghani, H. Zamani, Al. Severyn, J. Kamps, and W. B. Croft. "Neural Ranking Models with Weak Supervision", SIGIR'17.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, M. de Rijke. "Share your Model instead of your Data: Privacy-Preserving Mimic Learning for Ranking'', Neu-IR'17.
Mostafa Dehghani, S. Rothe, E. Alfonseca and P. Fleury. "Learning to Attend, Copy, and Generate for Session-Based Query Suggestion'', CIKM'17.
Chapter 5: Learning from Samples of Variable Quality
Mostafa Dehghani, A. Mehrjou, S. Gouws, J Kamps, B Schölkopf. "Fidelity-Weighted Learning", ICLR'18.
Mostafa Dehghani, A. Severyn, S. Rothe, and J. Kamps. "Learning to Learn from Weak Supervision by Full Supervision", NIPS2017 workshop on Meta-Learning 2017.
Mostafa Dehghani, A. Severyn, S. Rothe, and J. Kamps. "Avoiding Your Teacher’s Mistake: Training Neural Networks with Controlled Weak Supervision", arXiv preprint arXiv:1711.00313 (2017). 

PART III: Injecting Inductive Biases for Data Efficiency
In this part, we discuss injecting inductive biases into learning algorithms as a way to help them to come up with more generalizable solutions when they are provided with limited observations. We further discuss how we can improve the generalization of Transformers, the self-attentive feedforward networks for sequence modeling, by introducing a recurrent inductive bias into their architecture.
Chapter 6: Recurrent Inductive Bias for Transformers
Mostafa Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. "Universal Transformers", ICLR'19.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, M. de Rijke. "Learning to Transform, Combine, and Reason in Open-Domain Question Answering'', WSDM'19.


\section{Origins}
Next, we present the origins of each chapter in terms of the scholarly papers they are
based on.

PART I: Structure of the Data as Prior Knowledge
In this part, we explore how taking the general structured of the data into account can help to estimate representations that capture only the significant features when the data is noisy and highly variant. We break this into two chapters:
Chapter 2: Learning neither General, nor Specific, but Significant Representations 
Mostafa Dehghani, H. Azarbonyad, J. Kamps, D. Hiemstra, and M. Marx. "Luhn Revisited: Significant Words Language Models", CIKM'16.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx "Generalized Group Profiling for Content Customization", CHIIR'16.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx, "Significant Words Language Models for Contextual Suggestion", TREC  2016.
Mostafa Dehghani, "Significant Words Representations of Entities", SIGIR-Doctoral Consortium'16 -- (SIGIR Doctoral Consortium Award).
Chapter 3:  Representational Separability for Hierarchically Structured Data
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx, "On Horizontal and Vertical Separation in Hierarchical Text Classification", ICTIR'16 (Best Paper Award).
Mostafa Dehghani, H. Azarbonyad, J. Kamps, and M. Marx, "Two-Way Parsimonious Classification Models for Evolving Hierarchies", CLEF'16 (Best Paper Honorable Mention).

PART II: Learning with Weak Supervision.
In this part, we study how human can supervise machine learning systems, by labeling training data programmatically instead of labeling by hand and discuss how to design neural networks that learn to go beyond the imperfection in the weakly annotated data, for instance by meta learning the quality of the labels and modulating the learning process based on the label quality. We break this into two chapters:
Chapter 4: Learning from Pseudo-Labels
Mostafa Dehghani, H. Zamani, Al. Severyn, J. Kamps, and W. B. Croft. "Neural Ranking Models with Weak Supervision", SIGIR'17.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, M. de Rijke. "Share your Model instead of your Data: Privacy-Preserving Mimic Learning for Ranking'', Neu-IR'17.
Mostafa Dehghani, S. Rothe, E. Alfonseca and P. Fleury. "Learning to Attend, Copy, and Generate for Session-Based Query Suggestion'', CIKM'17.
Chapter 5: Learning from Samples of Variable Quality
Mostafa Dehghani, A. Mehrjou, S. Gouws, J Kamps, B Schölkopf. "Fidelity-Weighted Learning", ICLR'18.
Mostafa Dehghani, A. Severyn, S. Rothe, and J. Kamps. "Learning to Learn from Weak Supervision by Full Supervision", NIPS2017 workshop on Meta-Learning 2017.
Mostafa Dehghani, A. Severyn, S. Rothe, and J. Kamps. "Avoiding Your Teacher’s Mistake: Training Neural Networks with Controlled Weak Supervision", arXiv preprint arXiv:1711.00313 (2017). 

PART III: Injecting Inductive Biases for Data Efficiency
In this part, we discuss injecting inductive biases into learning algorithms as a way to help them to come up with more generalizable solutions when they are provided with limited observations. We further discuss how we can improve the generalization of Transformers, the self-attentive feedforward networks for sequence modeling, by introducing a recurrent inductive bias into their architecture.
Chapter 6: Recurrent Inductive Bias for Transformers
Mostafa Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. "Universal Transformers", ICLR'19.
Mostafa Dehghani, H. Azarbonyad, J. Kamps, M. de Rijke. "Learning to Transform, Combine, and Reason in Open-Domain Question Answering'', WSDM'19.



% \resq{C2.1}

