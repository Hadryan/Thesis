\samenvatting
\vspace{-30pt}
% \begin{flushright} 
% \textbf{Leren met imperfect toezicht voor taalbegrip} 
\textbf{Leren met imperfecte supervisie voor taalbegrip} 
% \end{flushright}
\vspace{-10pt}\par\noindent\rule{\textwidth}{0.4pt}

Mensen leren complexe problemen op te lossen en onderliggende concepten en relaties te ontdekken op basis van beperkte, ruizige of inconsistente observaties en kunnen daarmee succesvol generaliseren.
Dit berust grotendeels op het argument van de armoede van de stimulus, ofwel het probleem van Plato: "Hoe weten we zoveel wanneer het beschikbare bewijs zo mager is?"

Daarentegen is het succes van machinaal leren vaak sterk gecorreleerd met de hoeveelheid beschikbare hoge kwaliteit, gelabelde data. Machinaal leren met beperkt gelabelde data of met imperfecte labels blijft een belangrijke uitdaging.
In de praktijk is echter vaak geen grote hoeveelheid gelabelde data van hoge kwaliteit beschikbaar.
Hierdoor ontstaat een toenemende behoefte om modellen te ontwerpen die complexe taken met imperfecte supervisie kunnen oplossen, oftewel waarbij het leerproces op imperfect gelabelde data is gebaseerd.

Bij het ontwerpen van algoritmen lijken methodes die enkel op basis van eerdere ervaringen leren geen generaliserende oplossingen op te leveren.
Vergelijkbaar met menselijk leren, waarbij een deel van de kennis van nature aanwezig is, kunnen ook leeralgoritmen mogelijk beter generaliseren wanneer een deel van de kennis gecodeerd wordt in het algoritme in de vorm van voorkennis.

In dit proefschrift richten we ons op het probleem van de armoede van de stimulus voor machinaal leren.
Wij stellen dat zelfs beperkte, ruizige signalen nuttige informatie kunnen bevatten.
Zulke signalen kunnen, samen met voorkennis die wordt gecodeerd in de algoritmen, gebruikt worden om complexe problemen op te lossen.
We verbeteren het leerproces met imperfecte supervisie door (i) voorkennis in leeralgoritmen toe te passen, (ii) data kunstmatig aan te vullen en te leren hoe de data beter te gebruiken is, en (iii) inductieve bias in de leeralgoritmen te introduceren.
Deze algemene ideeën zijn in feite de belangrijkste ingrediënten voor het bouwen van leeralgoritmen die beter kunnen generaliseren bij beperkt gelabelde data.

We concentreren ons op het begrijpen van en redeneren met taal, \'e\'en van de unieke cognitieve vaardigheden van de mens, evenals een cruciaal probleem in kunstmatige intelligentie.
We proberen het leerproces te verbeteren op principiële manieren die ad-hoc en domein- of taakspecifieke trucs vermijden.
We onderzoeken onze ideeën aan de hand van een breed scala van taken zoals sequentie modelleren en taalbegrip.

% Mensen kunnen complexe problemen leren op te lossen en kunnen onderliggende concepten en relaties blootleggen op basis van zeer beperkte of inconsistente observaties met mogelijk veel ruis en kunnen op basis daarvan succesvol generaliseren.
% Dit berust grotendeels op het argument van de armoede van de stimulus, of wat soms het probleem van Plato wordt genoemd: "Hoe weten we zoveel wanneer het beschikbare bewijs zo mager is?"

% Daarentegen is het succes van machinaal leren vaak sterk gecorreleerd met de hoeveelheid beschikbare gelabelde gegevens van hoge kwaliteit en machinaal leren met weinig gelabelde gegevens of met toezicht van lage kwaliteit blijft een belangrijke uitdaging.
% In de praktijk zijn echter voor veel toepassingen geen grote hoeveelheid gelabelde gegevens van hoge kwaliteit beschikbaar.
% Dit benadrukt de toenemende behoefte voor het bouwen van modellen die complexe taken met imperfecte supervisie kunnen oplossen, dat wil zeggen waar het leerproces op imperfecte gelabelde gegevens is gebaseerd.

% Bij het ontwerpen van leeralgoritmen lijkt puur datagestuurd leren, methodes die enkel op basis van eerdere ervaringen leren, geen generaliseerbare oplossingen oplevert.
% Vergelijkbaar met menselijk leren, waarbij een deel van de kennis van nature aanwezig is, kunnen ook leeralgoritmen mogelijk beter generaliseren wanneer een deel van de kennis gecodeerd wordt in het leeralgoritme in de vorm van zwakke of sterke voorkennis.

% In dit proefschrift richten we ons op het probleem van de armoede van de stimulus voor machinaal leren.
% Wij beweren dat zelfs beperkte signalen met ruis veel geldige informatie kunnen bevatten.
% Zulke signalen kunnen, samen met voorkennis die wordt gecodeerd in de leeralgoritmen, gebruikt worden om complexe problemen op te lossen.
% We verbeteren het leerproces met imperfecte supervisie door (i) voorkennis in leeralgoritmen toe te passen, (ii) data aan te passen en te leren hoe de data beter te gebruiken is, en (iii) inductieve bias in de leeralgoritmen te introduceren.
% Deze algemene ideeën zijn in feite de belangrijkste ingrediënten voor het bouwen van leeralgoritmen die beter kunnen generaliseren bij weinig gelabelde gegevens.

% We concentreren ons op het begrijpen en redeneren met natuurlijke taal, één van de buitengewone cognitieve vaardigheden van mensen, evenals een cruciaal probleem in kunstmatige intelligentie.
% We proberen het leerproces te verbeteren op meer principiële manieren dan ad-hoc en domein- of taakspecifieke trucs.
% We onderzoeken onze ideeën op een breed scala aan taken op het gebied van sequentie modellering en taalbegrip.